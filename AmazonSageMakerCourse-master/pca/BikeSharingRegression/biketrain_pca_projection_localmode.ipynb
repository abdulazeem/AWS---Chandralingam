{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.decomposition import PCA\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "import boto3\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Kaggle Bike Sharing Demand Dataset Preparation For PCA</h2>\n",
    "<h4>Use PCA to find new components to replace 'temp','atemp','humidity','windspeed' in both training and test datasets</h4>\n",
    "<h4>To download dataset, sign-in and download from this link: https://www.kaggle.com/c/bike-sharing-demand/data</h4>\n",
    "<br>\n",
    "Input Features: ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'day', 'dayofweek','hour', <b>'pca components'</b>]<br>\n",
    "Target Feature: [log1p('count')]<br>\n",
    "PCA Training: ['temp','atemp','humidity','windspeed']<br><br>\n",
    "\n",
    "Objective: <quote>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period (Ref: Kaggle.com)</quote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not going to use numeric features: 'temp','atemp','humidity','windspeed'\n",
    "# Instead, we are going to use new components (aka features) generated by PCA for model training and testing\n",
    "columns = ['count', 'season', 'holiday', 'workingday', 'weather','year', 'month', 'day', 'dayofweek','hour']\n",
    "\n",
    "# PCA Training\n",
    "colums_for_pca = ['temp','atemp','humidity','windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_normalized.csv')\n",
    "df_test = pd.read_csv('test_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find PCA\n",
    "pca = PCA(n_components=0.9) # Capture 90% total variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new components\n",
    "pca.fit(df[colums_for_pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of PCA Components\n",
    "print ('Variance: ', pca.n_components)\n",
    "print ('No. of components to keep: ', pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_pca(pca, df, columns):\n",
    "    transformed_data = pca.transform(df[columns])\n",
    "    \n",
    "    tcols = []\n",
    "    for i in range(pca.n_components_):       \n",
    "        tcols.append('component_' + str(i))\n",
    "    \n",
    "    print ('components:',tcols)\n",
    "    df_transformed = pd.DataFrame(transformed_data, columns=tcols)\n",
    "    \n",
    "    for col in df_transformed.columns:\n",
    "        df[col] = df_transformed[col]\n",
    "    \n",
    "    df.drop(columns, inplace=True, axis=1)\n",
    "    \n",
    "    return tcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = transform_with_pca(pca, df, colums_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_with_pca(pca, df_test, colums_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_cols:\n",
    "    columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Test Set\n",
    "### Target Variable as first column followed by input features\n",
    "### Training, Validation files do not have a column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training = 70% of the data\n",
    "# Validation = 30% of the data\n",
    "# Randomize the datset\n",
    "np.random.seed(5)\n",
    "l = list(df.index)\n",
    "np.random.shuffle(l)\n",
    "df = df.iloc[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.shape[0]\n",
    "train = int(.7 * rows)\n",
    "test = int(.3 * rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Set\n",
    "df[:train].to_csv('bike_train_pca.csv'\n",
    "                          ,index=False,header=False\n",
    "                          ,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Validation Set\n",
    "df[train:].to_csv('bike_validation_pca.csv'\n",
    "                          ,index=False,header=False\n",
    "                          ,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data has only input features\n",
    "df_test.to_csv('bike_test_pca.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Column List\n",
    "with open('bike_train_column_list_pca.txt','w') as f:\n",
    "    f.write(','.join(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
