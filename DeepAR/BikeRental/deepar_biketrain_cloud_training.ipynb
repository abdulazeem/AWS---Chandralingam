{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a good base job name when building different models\n",
    "# It will help in identifying trained models and endpoints\n",
    "with_categories = False\n",
    "if with_categories:\n",
    "    base_job_name = 'deepar-biketrain-with-categories'\n",
    "else:\n",
    "    base_job_name = 'deepar-biketrain-no-categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket = 'chandra-ml-sagemaker'\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "if with_categories:\n",
    "    s3_data_path = \"{}/{}/data_with_categories\".format(bucket, prefix)\n",
    "else:\n",
    "    s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "    \n",
    "\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chandra-ml-sagemaker/deepar/bikerental/data',\n",
       " 'chandra-ml-sagemaker/deepar/bikerental/output')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "if with_categories:\n",
    "    write_to_s3('train_with_categories.json',bucket,'deepar/bikerental/data_with_categories/train/train_with_categories.json')\n",
    "    write_to_s3('test_with_categories.json',bucket,'deepar/bikerental/data_with_categories/test/test_with_categories.json')\n",
    "else:\n",
    "    write_to_s3('train.json',bucket,'deepar/bikerental/data/train/train.json')\n",
    "    write_to_s3('test.json',bucket,'deepar/bikerental/data/test/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer have to maintain a mapping of container images by region\n",
    "# Simply use the convenience method provided by sagemaker\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Free Tier (if you are still under free-tier)\n",
    "# At this time, m4.xlarge is offered as part of 2 months free tier\n",
    "# https://aws.amazon.com/sagemaker/pricing/\n",
    "# If you are outside of free-tier, you can also use ml.m5.xlarge  (newer generation instance)\n",
    "# In this example, I am using ml.m5.xlarge for training\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '40',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://chandra-ml-sagemaker/deepar/bikerental/data/train/',\n",
       " 'test': 's3://chandra-ml-sagemaker/deepar/bikerental/data/test/'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 22:12:59 Starting - Starting the training job...\n",
      "2020-04-23 22:13:01 Starting - Launching requested ML instances......\n",
      "2020-04-23 22:14:05 Starting - Preparing the instances for training......\n",
      "2020-04-23 22:15:05 Downloading - Downloading input data...\n",
      "2020-04-23 22:15:52 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'288', u'epochs': u'400', u'time_freq': u'H', u'context_length': u'288', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'288', u'time_freq': u'H', u'context_length': u'288', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Integer time series\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] number of time series: 3\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] mean target length: 16968\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] min/mean/max target: 0.0/79.5729608675/977.0\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] mean abs(target): 79.5729608675\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Small number of time series. Doing 214 passes over dataset with prob 0.996884735202 per epoch.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Integer time series\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] number of time series: 3\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] number of observations: 51768\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] mean target length: 17256\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] min/mean/max target: 0.0/80.5700819039/977.0\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] mean abs(target): 80.5700819039\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] contains missing values: yes (36.9%)\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] nvidia-smi took: 0.025151014328 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:15:55 INFO 139961471010624] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 24243.430137634277, \"sum\": 24243.430137634277, \"min\": 24243.430137634277}}, \"EndTime\": 1587680179.783077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680155.538763}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:19 INFO 139961471010624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 27410.2201461792, \"sum\": 27410.2201461792, \"min\": 27410.2201461792}}, \"EndTime\": 1587680182.949114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680179.783633}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:28 INFO 139961471010624] Epoch[0] Batch[0] avg_epoch_loss=4.261261\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.26126146317\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:34 INFO 139961471010624] Epoch[0] Batch[5] avg_epoch_loss=4.018581\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.01858063539\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:34 INFO 139961471010624] Epoch[0] Batch [5]#011Speed: 57.57 samples/sec#011loss=4.018581\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] Epoch[0] Batch[10] avg_epoch_loss=3.863635\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=3.67770042419\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] Epoch[0] Batch [10]#011Speed: 60.31 samples/sec#011loss=3.677700\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 16767.143964767456, \"sum\": 16767.143964767456, \"min\": 16767.143964767456}}, \"EndTime\": 1587680199.716497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680182.94925}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=39.0642426818 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.86363508485\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:39 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_938964d6-1c85-4e11-9f5f-d664854cd05c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 234.23004150390625, \"sum\": 234.23004150390625, \"min\": 234.23004150390625}}, \"EndTime\": 1587680199.951208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680199.716572}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:45 INFO 139961471010624] Epoch[1] Batch[0] avg_epoch_loss=3.692782\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:45 INFO 139961471010624] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.69278240204\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:50 INFO 139961471010624] Epoch[1] Batch[5] avg_epoch_loss=3.545679\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:50 INFO 139961471010624] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.5456785361\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:50 INFO 139961471010624] Epoch[1] Batch [5]#011Speed: 61.58 samples/sec#011loss=3.545679\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14666.584968566895, \"sum\": 14666.584968566895, \"min\": 14666.584968566895}}, \"EndTime\": 1587680214.61791, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680199.951267}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.8408358405 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.47158193588\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:16:54 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_d4254777-3d05-4e47-99ab-ea0950f9e4bb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 249.27306175231934, \"sum\": 249.27306175231934, \"min\": 249.27306175231934}}, \"EndTime\": 1587680214.867642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680214.617985}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:17:00 INFO 139961471010624] Epoch[2] Batch[0] avg_epoch_loss=3.544548\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.54454803467\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:05 INFO 139961471010624] Epoch[2] Batch[5] avg_epoch_loss=3.488328\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.48832829793\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:05 INFO 139961471010624] Epoch[2] Batch [5]#011Speed: 60.72 samples/sec#011loss=3.488328\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:09 INFO 139961471010624] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14779.155015945435, \"sum\": 14779.155015945435, \"min\": 14779.155015945435}}, \"EndTime\": 1587680229.646922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680214.867705}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:09 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.0332925259 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:09 INFO 139961471010624] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:09 INFO 139961471010624] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.47926428318\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:09 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:14 INFO 139961471010624] Epoch[3] Batch[0] avg_epoch_loss=3.392361\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:14 INFO 139961471010624] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.39236068726\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:20 INFO 139961471010624] Epoch[3] Batch[5] avg_epoch_loss=3.411909\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.41190910339\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:20 INFO 139961471010624] Epoch[3] Batch [5]#011Speed: 61.66 samples/sec#011loss=3.411909\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14751.251935958862, \"sum\": 14751.251935958862, \"min\": 14751.251935958862}}, \"EndTime\": 1587680244.398604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680229.646989}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0978162482 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.39726977348\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:24 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_b2bd27d0-92e7-43ec-9422-96e846c2e763-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.7149829864502, \"sum\": 211.7149829864502, \"min\": 211.7149829864502}}, \"EndTime\": 1587680244.610916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680244.398676}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:29 INFO 139961471010624] Epoch[4] Batch[0] avg_epoch_loss=3.367940\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.36794042587\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:35 INFO 139961471010624] Epoch[4] Batch[5] avg_epoch_loss=3.378979\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:35 INFO 139961471010624] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.37897948424\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:35 INFO 139961471010624] Epoch[4] Batch [5]#011Speed: 60.97 samples/sec#011loss=3.378979\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] Epoch[4] Batch[10] avg_epoch_loss=3.373427\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=3.36676464081\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] Epoch[4] Batch [10]#011Speed: 60.65 samples/sec#011loss=3.366765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15857.01298713684, \"sum\": 15857.01298713684, \"min\": 15857.01298713684}}, \"EndTime\": 1587680260.468048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680244.610972}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.4415267679 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.37342728268\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:40 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_24c023e2-a709-4f30-8b17-c5a27adff020-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 213.07086944580078, \"sum\": 213.07086944580078, \"min\": 213.07086944580078}}, \"EndTime\": 1587680260.68161, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680260.468112}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:46 INFO 139961471010624] Epoch[5] Batch[0] avg_epoch_loss=3.260239\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.26023888588\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:51 INFO 139961471010624] Epoch[5] Batch[5] avg_epoch_loss=3.305167\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.30516735713\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:51 INFO 139961471010624] Epoch[5] Batch [5]#011Speed: 61.21 samples/sec#011loss=3.305167\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] Epoch[5] Batch[10] avg_epoch_loss=3.275888\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=3.24075379372\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] Epoch[5] Batch [10]#011Speed: 61.00 samples/sec#011loss=3.240754\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15800.382137298584, \"sum\": 15800.382137298584, \"min\": 15800.382137298584}}, \"EndTime\": 1587680276.482112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680260.681675}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.0999834244 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.27588846467\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:17:56 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_1b638fe8-8c80-448a-b579-d6fd3344013d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 217.61608123779297, \"sum\": 217.61608123779297, \"min\": 217.61608123779297}}, \"EndTime\": 1587680276.700158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680276.482171}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:02 INFO 139961471010624] Epoch[6] Batch[0] avg_epoch_loss=3.147259\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.14725923538\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:07 INFO 139961471010624] Epoch[6] Batch[5] avg_epoch_loss=3.140200\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.14020029704\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:07 INFO 139961471010624] Epoch[6] Batch [5]#011Speed: 60.52 samples/sec#011loss=3.140200\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] Epoch[6] Batch[10] avg_epoch_loss=3.077275\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=3.00176482201\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] Epoch[6] Batch [10]#011Speed: 60.77 samples/sec#011loss=3.001765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15880.438089370728, \"sum\": 15880.438089370728, \"min\": 15880.438089370728}}, \"EndTime\": 1587680292.580713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680276.700217}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.3084471665 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.07727508111\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:12 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_65f41a55-7d92-400b-a0c9-188986fa6180-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 229.24518585205078, \"sum\": 229.24518585205078, \"min\": 229.24518585205078}}, \"EndTime\": 1587680292.81034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680292.580775}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:18:18 INFO 139961471010624] Epoch[7] Batch[0] avg_epoch_loss=3.071953\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.07195281982\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:23 INFO 139961471010624] Epoch[7] Batch[5] avg_epoch_loss=3.048494\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.0484940211\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:23 INFO 139961471010624] Epoch[7] Batch [5]#011Speed: 61.60 samples/sec#011loss=3.048494\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14693.672895431519, \"sum\": 14693.672895431519, \"min\": 14693.672895431519}}, \"EndTime\": 1587680307.504144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680292.810413}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.1058619542 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.01903851032\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:27 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_452ad5a0-cb87-4cbb-8464-9fb2ed58f3b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 212.94188499450684, \"sum\": 212.94188499450684, \"min\": 212.94188499450684}}, \"EndTime\": 1587680307.717612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680307.504209}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:33 INFO 139961471010624] Epoch[8] Batch[0] avg_epoch_loss=2.993235\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.99323511124\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:38 INFO 139961471010624] Epoch[8] Batch[5] avg_epoch_loss=2.991030\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.99103013674\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:38 INFO 139961471010624] Epoch[8] Batch [5]#011Speed: 60.47 samples/sec#011loss=2.991030\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:42 INFO 139961471010624] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14832.82208442688, \"sum\": 14832.82208442688, \"min\": 14832.82208442688}}, \"EndTime\": 1587680322.550552, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680307.717671}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:42 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.7202493404 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:42 INFO 139961471010624] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.02480540276\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:42 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:47 INFO 139961471010624] Epoch[9] Batch[0] avg_epoch_loss=3.023144\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.02314400673\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:53 INFO 139961471010624] Epoch[9] Batch[5] avg_epoch_loss=3.013930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.01392956575\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:53 INFO 139961471010624] Epoch[9] Batch [5]#011Speed: 61.18 samples/sec#011loss=3.013930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] Epoch[9] Batch[10] avg_epoch_loss=3.102820\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.20948839188\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] Epoch[9] Batch [10]#011Speed: 60.92 samples/sec#011loss=3.209488\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15813.104152679443, \"sum\": 15813.104152679443, \"min\": 15813.104152679443}}, \"EndTime\": 1587680338.364113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680322.550617}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.6622444358 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.10281994126\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:18:58 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:03 INFO 139961471010624] Epoch[10] Batch[0] avg_epoch_loss=3.062101\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.06210136414\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:08 INFO 139961471010624] Epoch[10] Batch[5] avg_epoch_loss=2.997220\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.99721992016\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:08 INFO 139961471010624] Epoch[10] Batch [5]#011Speed: 60.96 samples/sec#011loss=2.997220\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14805.443048477173, \"sum\": 14805.443048477173, \"min\": 14805.443048477173}}, \"EndTime\": 1587680353.169891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680338.364173}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2814782427 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.98250558376\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:13 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_53f962e5-b5b5-4ae5-9494-9a13476d5949-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.93289756774902, \"sum\": 211.93289756774902, \"min\": 211.93289756774902}}, \"EndTime\": 1587680353.382389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680353.169949}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:18 INFO 139961471010624] Epoch[11] Batch[0] avg_epoch_loss=3.061182\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.06118202209\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:23 INFO 139961471010624] Epoch[11] Batch[5] avg_epoch_loss=2.955110\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.9551097552\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:23 INFO 139961471010624] Epoch[11] Batch [5]#011Speed: 61.49 samples/sec#011loss=2.955110\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14726.089000701904, \"sum\": 14726.089000701904, \"min\": 14726.089000701904}}, \"EndTime\": 1587680368.108596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680353.382451}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.4227814095 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.91738853455\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:28 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_76aba951-ea6f-45ca-8ab8-3b786a5f7a03-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 227.25486755371094, \"sum\": 227.25486755371094, \"min\": 227.25486755371094}}, \"EndTime\": 1587680368.336335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680368.108671}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:33 INFO 139961471010624] Epoch[12] Batch[0] avg_epoch_loss=2.918189\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.91818928719\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:19:38 INFO 139961471010624] Epoch[12] Batch[5] avg_epoch_loss=2.906964\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.90696430206\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:38 INFO 139961471010624] Epoch[12] Batch [5]#011Speed: 60.95 samples/sec#011loss=2.906964\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14785.629987716675, \"sum\": 14785.629987716675, \"min\": 14785.629987716675}}, \"EndTime\": 1587680383.122089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680368.336399}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2703382143 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.86870727539\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:43 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_917997ea-3bcd-4ade-8695-2ff9a9f7e2c9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 209.15508270263672, \"sum\": 209.15508270263672, \"min\": 209.15508270263672}}, \"EndTime\": 1587680383.33194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680383.122207}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:48 INFO 139961471010624] Epoch[13] Batch[0] avg_epoch_loss=2.943574\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.94357395172\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:53 INFO 139961471010624] Epoch[13] Batch[5] avg_epoch_loss=2.903192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.90319164594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:53 INFO 139961471010624] Epoch[13] Batch [5]#011Speed: 61.15 samples/sec#011loss=2.903192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:58 INFO 139961471010624] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14748.522996902466, \"sum\": 14748.522996902466, \"min\": 14748.522996902466}}, \"EndTime\": 1587680398.080577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680383.331999}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:58 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.2920040915 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:58 INFO 139961471010624] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.95382261276\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:19:58 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:03 INFO 139961471010624] Epoch[14] Batch[0] avg_epoch_loss=2.857470\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.85746955872\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:08 INFO 139961471010624] Epoch[14] Batch[5] avg_epoch_loss=2.882535\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.88253506025\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:08 INFO 139961471010624] Epoch[14] Batch [5]#011Speed: 60.90 samples/sec#011loss=2.882535\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:12 INFO 139961471010624] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14790.129899978638, \"sum\": 14790.129899978638, \"min\": 14790.129899978638}}, \"EndTime\": 1587680412.871196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680398.080642}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:12 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.649126979 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:12 INFO 139961471010624] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.85166752338\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:12 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:13 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_b3891de8-150b-489c-8134-bbb6b8c24b09-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.37309074401855, \"sum\": 211.37309074401855, \"min\": 211.37309074401855}}, \"EndTime\": 1587680413.083083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680412.871262}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:18 INFO 139961471010624] Epoch[15] Batch[0] avg_epoch_loss=2.843410\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.84340977669\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:23 INFO 139961471010624] Epoch[15] Batch[5] avg_epoch_loss=2.889350\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.88934977849\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:23 INFO 139961471010624] Epoch[15] Batch [5]#011Speed: 61.86 samples/sec#011loss=2.889350\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:27 INFO 139961471010624] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14671.8168258667, \"sum\": 14671.8168258667, \"min\": 14671.8168258667}}, \"EndTime\": 1587680427.755024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680413.083141}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:27 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.9391331504 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:27 INFO 139961471010624] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.88165867329\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:27 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:33 INFO 139961471010624] Epoch[16] Batch[0] avg_epoch_loss=2.844587\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.84458661079\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:38 INFO 139961471010624] Epoch[16] Batch[5] avg_epoch_loss=2.863824\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.863824447\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:38 INFO 139961471010624] Epoch[16] Batch [5]#011Speed: 61.39 samples/sec#011loss=2.863824\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14689.656972885132, \"sum\": 14689.656972885132, \"min\": 14689.656972885132}}, \"EndTime\": 1587680442.445208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680427.755102}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.253238205 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.85086755753\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:42 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_24263f0e-d7e2-4141-baab-74606da4c8b6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 214.93887901306152, \"sum\": 214.93887901306152, \"min\": 214.93887901306152}}, \"EndTime\": 1587680442.660617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680442.445273}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:47 INFO 139961471010624] Epoch[17] Batch[0] avg_epoch_loss=2.750237\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.7502374649\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:20:53 INFO 139961471010624] Epoch[17] Batch[5] avg_epoch_loss=2.853821\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.85382080078\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:53 INFO 139961471010624] Epoch[17] Batch [5]#011Speed: 61.79 samples/sec#011loss=2.853821\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14636.117935180664, \"sum\": 14636.117935180664, \"min\": 14636.117935180664}}, \"EndTime\": 1587680457.29685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680442.660674}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0873894974 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.84977357388\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:20:57 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_5076c279-12a5-4aa1-94be-0163c59393c1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 218.98293495178223, \"sum\": 218.98293495178223, \"min\": 218.98293495178223}}, \"EndTime\": 1587680457.516304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680457.296916}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:02 INFO 139961471010624] Epoch[18] Batch[0] avg_epoch_loss=2.698833\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.69883275032\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:08 INFO 139961471010624] Epoch[18] Batch[5] avg_epoch_loss=2.806490\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.80649018288\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:08 INFO 139961471010624] Epoch[18] Batch [5]#011Speed: 60.46 samples/sec#011loss=2.806490\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] Epoch[18] Batch[10] avg_epoch_loss=2.811489\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.81748800278\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] Epoch[18] Batch [10]#011Speed: 60.44 samples/sec#011loss=2.817488\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15939.918994903564, \"sum\": 15939.918994903564, \"min\": 15939.918994903564}}, \"EndTime\": 1587680473.456344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680457.516365}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9071178209 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.81148919192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:13 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_8b244aaa-780f-4c1d-8810-c0405674433e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 219.29597854614258, \"sum\": 219.29597854614258, \"min\": 219.29597854614258}}, \"EndTime\": 1587680473.676228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680473.456406}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:19 INFO 139961471010624] Epoch[19] Batch[0] avg_epoch_loss=2.631122\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:19 INFO 139961471010624] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.63112187386\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:24 INFO 139961471010624] Epoch[19] Batch[5] avg_epoch_loss=2.699921\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:24 INFO 139961471010624] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.69992105166\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:24 INFO 139961471010624] Epoch[19] Batch [5]#011Speed: 61.38 samples/sec#011loss=2.699921\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] Epoch[19] Batch[10] avg_epoch_loss=2.704239\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.70942139626\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] Epoch[19] Batch [10]#011Speed: 60.96 samples/sec#011loss=2.709421\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15791.460037231445, \"sum\": 15791.460037231445, \"min\": 15791.460037231445}}, \"EndTime\": 1587680489.467801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680473.676283}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.0346010571 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.70423939011\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:29 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_3ee949d5-6e9a-4bd8-be9a-c9a481bc2db8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.2109661102295, \"sum\": 211.2109661102295, \"min\": 211.2109661102295}}, \"EndTime\": 1587680489.679581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680489.46786}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:34 INFO 139961471010624] Epoch[20] Batch[0] avg_epoch_loss=2.864929\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.86492872238\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:40 INFO 139961471010624] Epoch[20] Batch[5] avg_epoch_loss=2.771114\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.77111363411\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:40 INFO 139961471010624] Epoch[20] Batch [5]#011Speed: 60.34 samples/sec#011loss=2.771114\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14820.695161819458, \"sum\": 14820.695161819458, \"min\": 14820.695161819458}}, \"EndTime\": 1587680504.500387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680489.679639}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=39.9438952726 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.68627743721\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:44 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_2408e623-d316-44a5-a45b-902b63098b38-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 224.5330810546875, \"sum\": 224.5330810546875, \"min\": 224.5330810546875}}, \"EndTime\": 1587680504.725467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680504.500451}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:50 INFO 139961471010624] Epoch[21] Batch[0] avg_epoch_loss=2.860647\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:50 INFO 139961471010624] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.86064696312\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:55 INFO 139961471010624] Epoch[21] Batch[5] avg_epoch_loss=2.841672\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:55 INFO 139961471010624] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.84167170525\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:21:55 INFO 139961471010624] Epoch[21] Batch [5]#011Speed: 61.17 samples/sec#011loss=2.841672\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] Epoch[21] Batch[10] avg_epoch_loss=2.829593\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.81509947777\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] Epoch[21] Batch [10]#011Speed: 60.90 samples/sec#011loss=2.815099\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15815.82498550415, \"sum\": 15815.82498550415, \"min\": 15815.82498550415}}, \"EndTime\": 1587680520.541405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680504.725526}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.2874110865 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.82959342003\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:00 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:05 INFO 139961471010624] Epoch[22] Batch[0] avg_epoch_loss=2.644729\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.64472913742\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:11 INFO 139961471010624] Epoch[22] Batch[5] avg_epoch_loss=2.756785\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.75678519408\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:11 INFO 139961471010624] Epoch[22] Batch [5]#011Speed: 60.38 samples/sec#011loss=2.756785\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] Epoch[22] Batch[10] avg_epoch_loss=2.788521\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=2.82660388947\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] Epoch[22] Batch [10]#011Speed: 60.94 samples/sec#011loss=2.826604\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15931.036949157715, \"sum\": 15931.036949157715, \"min\": 15931.036949157715}}, \"EndTime\": 1587680536.472826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680520.541507}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.1772554081 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.78852096471\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:16 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:21 INFO 139961471010624] Epoch[23] Batch[0] avg_epoch_loss=2.533490\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:21 INFO 139961471010624] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.53348994255\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:27 INFO 139961471010624] Epoch[23] Batch[5] avg_epoch_loss=2.732646\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.73264618715\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:27 INFO 139961471010624] Epoch[23] Batch [5]#011Speed: 60.78 samples/sec#011loss=2.732646\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:31 INFO 139961471010624] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14833.730936050415, \"sum\": 14833.730936050415, \"min\": 14833.730936050415}}, \"EndTime\": 1587680551.30699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680536.472883}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:31 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6726280105 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:31 INFO 139961471010624] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.70826718807\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:31 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:36 INFO 139961471010624] Epoch[24] Batch[0] avg_epoch_loss=2.720021\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.72002053261\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:41 INFO 139961471010624] Epoch[24] Batch[5] avg_epoch_loss=2.730513\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.73051273823\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:41 INFO 139961471010624] Epoch[24] Batch [5]#011Speed: 60.08 samples/sec#011loss=2.730513\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:46 INFO 139961471010624] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14852.147102355957, \"sum\": 14852.147102355957, \"min\": 14852.147102355957}}, \"EndTime\": 1587680566.159713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680551.307091}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:46 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.8891132412 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:46 INFO 139961471010624] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.73855464458\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:46 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:51 INFO 139961471010624] Epoch[25] Batch[0] avg_epoch_loss=2.796342\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.79634165764\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:56 INFO 139961471010624] Epoch[25] Batch[5] avg_epoch_loss=2.762730\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.7627300024\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:22:56 INFO 139961471010624] Epoch[25] Batch [5]#011Speed: 61.19 samples/sec#011loss=2.762730\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:00 INFO 139961471010624] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14761.205911636353, \"sum\": 14761.205911636353, \"min\": 14761.205911636353}}, \"EndTime\": 1587680580.921497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680566.159783}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:00 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.7468894987 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:00 INFO 139961471010624] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.77697110176\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:00 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:06 INFO 139961471010624] Epoch[26] Batch[0] avg_epoch_loss=2.756513\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.75651264191\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:11 INFO 139961471010624] Epoch[26] Batch[5] avg_epoch_loss=2.720867\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.72086735566\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:11 INFO 139961471010624] Epoch[26] Batch [5]#011Speed: 60.51 samples/sec#011loss=2.720867\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] Epoch[26] Batch[10] avg_epoch_loss=2.724529\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.72892227173\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] Epoch[26] Batch [10]#011Speed: 60.66 samples/sec#011loss=2.728922\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15931.26916885376, \"sum\": 15931.26916885376, \"min\": 15931.26916885376}}, \"EndTime\": 1587680596.853275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680580.921567}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.934187648 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.72452868115\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:16 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:23:22 INFO 139961471010624] Epoch[27] Batch[0] avg_epoch_loss=2.725409\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.72540926933\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:27 INFO 139961471010624] Epoch[27] Batch[5] avg_epoch_loss=2.717329\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.71732902527\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:27 INFO 139961471010624] Epoch[27] Batch [5]#011Speed: 61.34 samples/sec#011loss=2.717329\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:31 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14726.186990737915, \"sum\": 14726.186990737915, \"min\": 14726.186990737915}}, \"EndTime\": 1587680611.579872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680596.853336}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:31 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.5087437818 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:31 INFO 139961471010624] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.73201580048\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:31 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:36 INFO 139961471010624] Epoch[28] Batch[0] avg_epoch_loss=2.612612\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.61261248589\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:42 INFO 139961471010624] Epoch[28] Batch[5] avg_epoch_loss=2.679269\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.67926943302\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:42 INFO 139961471010624] Epoch[28] Batch [5]#011Speed: 60.68 samples/sec#011loss=2.679269\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:46 INFO 139961471010624] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14792.912006378174, \"sum\": 14792.912006378174, \"min\": 14792.912006378174}}, \"EndTime\": 1587680626.373399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680611.580036}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:46 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.4384541513 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:46 INFO 139961471010624] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.71776306629\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:46 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:51 INFO 139961471010624] Epoch[29] Batch[0] avg_epoch_loss=2.685081\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.68508052826\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:56 INFO 139961471010624] Epoch[29] Batch[5] avg_epoch_loss=2.715791\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.71579102675\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:23:56 INFO 139961471010624] Epoch[29] Batch [5]#011Speed: 61.46 samples/sec#011loss=2.715791\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:01 INFO 139961471010624] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14942.301034927368, \"sum\": 14942.301034927368, \"min\": 14942.301034927368}}, \"EndTime\": 1587680641.3163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680626.373477}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:01 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9611107163 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:01 INFO 139961471010624] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.71891255379\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:01 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:06 INFO 139961471010624] Epoch[30] Batch[0] avg_epoch_loss=2.730592\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.73059248924\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:11 INFO 139961471010624] Epoch[30] Batch[5] avg_epoch_loss=2.656290\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.65629001458\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:11 INFO 139961471010624] Epoch[30] Batch [5]#011Speed: 60.84 samples/sec#011loss=2.656290\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14811.065196990967, \"sum\": 14811.065196990967, \"min\": 14811.065196990967}}, \"EndTime\": 1587680656.127901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680641.31637}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.7800300033 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.62321062088\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:16 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_ba191524-1388-4621-abc5-85d207efb579-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 208.33206176757812, \"sum\": 208.33206176757812, \"min\": 208.33206176757812}}, \"EndTime\": 1587680656.336815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680656.127972}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:21 INFO 139961471010624] Epoch[31] Batch[0] avg_epoch_loss=2.717848\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:21 INFO 139961471010624] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.71784830093\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:26 INFO 139961471010624] Epoch[31] Batch[5] avg_epoch_loss=2.732959\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.73295927048\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:26 INFO 139961471010624] Epoch[31] Batch [5]#011Speed: 61.23 samples/sec#011loss=2.732959\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] Epoch[31] Batch[10] avg_epoch_loss=2.754169\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=2.77962093353\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] Epoch[31] Batch [10]#011Speed: 60.18 samples/sec#011loss=2.779621\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15848.368167877197, \"sum\": 15848.368167877197, \"min\": 15848.368167877197}}, \"EndTime\": 1587680672.18552, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680656.337093}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.7075168322 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.75416911732\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:32 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:37 INFO 139961471010624] Epoch[32] Batch[0] avg_epoch_loss=2.658457\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.65845680237\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:42 INFO 139961471010624] Epoch[32] Batch[5] avg_epoch_loss=2.615856\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.61585613092\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:42 INFO 139961471010624] Epoch[32] Batch [5]#011Speed: 61.00 samples/sec#011loss=2.615856\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] Epoch[32] Batch[10] avg_epoch_loss=2.617665\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=2.61983599663\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] Epoch[32] Batch [10]#011Speed: 60.59 samples/sec#011loss=2.619836\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15829.503059387207, \"sum\": 15829.503059387207, \"min\": 15829.503059387207}}, \"EndTime\": 1587680688.015442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680672.18558}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.4937686517 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.61766516079\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:48 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_59becb56-f59d-4636-a528-d849975d8ec0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.9138240814209, \"sum\": 211.9138240814209, \"min\": 211.9138240814209}}, \"EndTime\": 1587680688.227792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680688.0155}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:24:53 INFO 139961471010624] Epoch[33] Batch[0] avg_epoch_loss=2.544800\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.54480004311\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:58 INFO 139961471010624] Epoch[33] Batch[5] avg_epoch_loss=2.637818\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.63781793912\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:24:58 INFO 139961471010624] Epoch[33] Batch [5]#011Speed: 61.39 samples/sec#011loss=2.637818\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] Epoch[33] Batch[10] avg_epoch_loss=2.645751\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.65526957512\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] Epoch[33] Batch [10]#011Speed: 60.55 samples/sec#011loss=2.655270\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15848.119020462036, \"sum\": 15848.119020462036, \"min\": 15848.119020462036}}, \"EndTime\": 1587680704.076025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680688.227848}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.0963552554 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.64575050094\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:04 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:09 INFO 139961471010624] Epoch[34] Batch[0] avg_epoch_loss=2.706961\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:09 INFO 139961471010624] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.7069606781\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:14 INFO 139961471010624] Epoch[34] Batch[5] avg_epoch_loss=2.739321\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:14 INFO 139961471010624] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.73932055632\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:14 INFO 139961471010624] Epoch[34] Batch [5]#011Speed: 61.42 samples/sec#011loss=2.739321\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:18 INFO 139961471010624] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14726.542949676514, \"sum\": 14726.542949676514, \"min\": 14726.542949676514}}, \"EndTime\": 1587680718.803045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680704.076082}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:18 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.8782717058 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:18 INFO 139961471010624] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.6609610796\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:18 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:24 INFO 139961471010624] Epoch[35] Batch[0] avg_epoch_loss=2.648248\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:24 INFO 139961471010624] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.64824819565\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:29 INFO 139961471010624] Epoch[35] Batch[5] avg_epoch_loss=2.729070\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.72906967004\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:29 INFO 139961471010624] Epoch[35] Batch [5]#011Speed: 61.95 samples/sec#011loss=2.729070\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] Epoch[35] Batch[10] avg_epoch_loss=2.713399\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.69459395409\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] Epoch[35] Batch [10]#011Speed: 61.23 samples/sec#011loss=2.694594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15717.1950340271, \"sum\": 15717.1950340271, \"min\": 15717.1950340271}}, \"EndTime\": 1587680734.520806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680718.803116}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.1012326743 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.71339889006\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:34 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:39 INFO 139961471010624] Epoch[36] Batch[0] avg_epoch_loss=2.652235\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.65223479271\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:45 INFO 139961471010624] Epoch[36] Batch[5] avg_epoch_loss=2.653865\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:45 INFO 139961471010624] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.65386513869\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:45 INFO 139961471010624] Epoch[36] Batch [5]#011Speed: 61.77 samples/sec#011loss=2.653865\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] Epoch[36] Batch[10] avg_epoch_loss=2.568052\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=2.465076828\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] Epoch[36] Batch [10]#011Speed: 61.09 samples/sec#011loss=2.465077\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15735.532999038696, \"sum\": 15735.532999038696, \"min\": 15735.532999038696}}, \"EndTime\": 1587680750.256818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680734.520868}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.735584528 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.5680522702\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:50 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_fa91d573-b378-4c85-a365-7c8013d7ad05-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 218.4278964996338, \"sum\": 218.4278964996338, \"min\": 218.4278964996338}}, \"EndTime\": 1587680750.47571, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680750.256882}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:55 INFO 139961471010624] Epoch[37] Batch[0] avg_epoch_loss=2.605141\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:25:55 INFO 139961471010624] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.60514140129\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:00 INFO 139961471010624] Epoch[37] Batch[5] avg_epoch_loss=2.616271\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.61627070109\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:00 INFO 139961471010624] Epoch[37] Batch [5]#011Speed: 61.51 samples/sec#011loss=2.616271\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] Epoch[37] Batch[10] avg_epoch_loss=2.692161\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.78323001862\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] Epoch[37] Batch [10]#011Speed: 60.44 samples/sec#011loss=2.783230\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15788.841009140015, \"sum\": 15788.841009140015, \"min\": 15788.841009140015}}, \"EndTime\": 1587680766.264678, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680750.475772}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3714514757 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.69216129997\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:06 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:26:11 INFO 139961471010624] Epoch[38] Batch[0] avg_epoch_loss=2.733540\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.73354029655\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:16 INFO 139961471010624] Epoch[38] Batch[5] avg_epoch_loss=2.666913\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.6669126749\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:16 INFO 139961471010624] Epoch[38] Batch [5]#011Speed: 61.19 samples/sec#011loss=2.666913\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] Epoch[38] Batch[10] avg_epoch_loss=2.702325\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=2.74482073784\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] Epoch[38] Batch [10]#011Speed: 60.65 samples/sec#011loss=2.744821\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15848.41799736023, \"sum\": 15848.41799736023, \"min\": 15848.41799736023}}, \"EndTime\": 1587680782.113634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680766.264737}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.571626889 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.70232543078\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:22 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:27 INFO 139961471010624] Epoch[39] Batch[0] avg_epoch_loss=2.636506\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.63650560379\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:32 INFO 139961471010624] Epoch[39] Batch[5] avg_epoch_loss=2.660427\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.66042677561\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:32 INFO 139961471010624] Epoch[39] Batch [5]#011Speed: 61.52 samples/sec#011loss=2.660427\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:36 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14732.8200340271, \"sum\": 14732.8200340271, \"min\": 14732.8200340271}}, \"EndTime\": 1587680796.846891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680782.113697}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:36 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2182329677 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:36 INFO 139961471010624] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.66042532921\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:36 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:42 INFO 139961471010624] Epoch[40] Batch[0] avg_epoch_loss=2.806270\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.80626964569\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:47 INFO 139961471010624] Epoch[40] Batch[5] avg_epoch_loss=2.678841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.6788409551\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:47 INFO 139961471010624] Epoch[40] Batch [5]#011Speed: 61.15 samples/sec#011loss=2.678841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] Epoch[40] Batch[10] avg_epoch_loss=2.617705\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=2.54434175491\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] Epoch[40] Batch [10]#011Speed: 60.63 samples/sec#011loss=2.544342\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15895.26915550232, \"sum\": 15895.26915550232, \"min\": 15895.26915550232}}, \"EndTime\": 1587680812.742591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680796.846991}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.3891474494 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.61770495501\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:52 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:58 INFO 139961471010624] Epoch[41] Batch[0] avg_epoch_loss=2.679734\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:26:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.6797337532\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:03 INFO 139961471010624] Epoch[41] Batch[5] avg_epoch_loss=2.633119\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.63311894735\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:03 INFO 139961471010624] Epoch[41] Batch [5]#011Speed: 61.19 samples/sec#011loss=2.633119\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:07 INFO 139961471010624] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14756.37698173523, \"sum\": 14756.37698173523, \"min\": 14756.37698173523}}, \"EndTime\": 1587680827.499367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680812.742652}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:07 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3542742516 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:07 INFO 139961471010624] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.64070520401\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:07 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:12 INFO 139961471010624] Epoch[42] Batch[0] avg_epoch_loss=2.722980\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.72298026085\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:18 INFO 139961471010624] Epoch[42] Batch[5] avg_epoch_loss=2.646077\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.64607691765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:18 INFO 139961471010624] Epoch[42] Batch [5]#011Speed: 61.56 samples/sec#011loss=2.646077\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] Epoch[42] Batch[10] avg_epoch_loss=2.634395\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.62037649155\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] Epoch[42] Batch [10]#011Speed: 60.72 samples/sec#011loss=2.620376\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15847.620964050293, \"sum\": 15847.620964050293, \"min\": 15847.620964050293}}, \"EndTime\": 1587680843.347491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680827.499439}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.2870056113 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.63439490578\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:23 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:28 INFO 139961471010624] Epoch[43] Batch[0] avg_epoch_loss=2.670242\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.67024183273\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:33 INFO 139961471010624] Epoch[43] Batch[5] avg_epoch_loss=2.630181\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.63018115362\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:33 INFO 139961471010624] Epoch[43] Batch [5]#011Speed: 61.32 samples/sec#011loss=2.630181\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] Epoch[43] Batch[10] avg_epoch_loss=2.625500\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.61988158226\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] Epoch[43] Batch [10]#011Speed: 60.08 samples/sec#011loss=2.619882\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15887.902975082397, \"sum\": 15887.902975082397, \"min\": 15887.902975082397}}, \"EndTime\": 1587680859.235724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680843.347553}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.296083267 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.62549953027\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:39 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:27:44 INFO 139961471010624] Epoch[44] Batch[0] avg_epoch_loss=2.737001\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:44 INFO 139961471010624] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.73700118065\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:49 INFO 139961471010624] Epoch[44] Batch[5] avg_epoch_loss=2.636045\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.63604545593\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:49 INFO 139961471010624] Epoch[44] Batch [5]#011Speed: 61.15 samples/sec#011loss=2.636045\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] Epoch[44] Batch[10] avg_epoch_loss=2.576937\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=2.50600771904\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] Epoch[44] Batch [10]#011Speed: 60.96 samples/sec#011loss=2.506008\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15799.777030944824, \"sum\": 15799.777030944824, \"min\": 15799.777030944824}}, \"EndTime\": 1587680875.036089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680859.235786}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.645917895 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.57693739371\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:27:55 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:00 INFO 139961471010624] Epoch[45] Batch[0] avg_epoch_loss=2.723778\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.72377753258\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:05 INFO 139961471010624] Epoch[45] Batch[5] avg_epoch_loss=2.680995\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.68099474907\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:05 INFO 139961471010624] Epoch[45] Batch [5]#011Speed: 60.97 samples/sec#011loss=2.680995\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:09 INFO 139961471010624] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14847.684860229492, \"sum\": 14847.684860229492, \"min\": 14847.684860229492}}, \"EndTime\": 1587680889.884183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680875.036149}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:09 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.8141522046 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:09 INFO 139961471010624] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:09 INFO 139961471010624] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.68208353519\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:09 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:15 INFO 139961471010624] Epoch[46] Batch[0] avg_epoch_loss=2.591514\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:15 INFO 139961471010624] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.59151363373\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:20 INFO 139961471010624] Epoch[46] Batch[5] avg_epoch_loss=2.632498\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.63249810537\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:20 INFO 139961471010624] Epoch[46] Batch [5]#011Speed: 61.08 samples/sec#011loss=2.632498\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] Epoch[46] Batch[10] avg_epoch_loss=2.647856\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=2.66628608704\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] Epoch[46] Batch [10]#011Speed: 60.49 samples/sec#011loss=2.666286\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15903.642177581787, \"sum\": 15903.642177581787, \"min\": 15903.642177581787}}, \"EndTime\": 1587680905.788345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680889.884254}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.3231346079 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.64785627885\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:25 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:31 INFO 139961471010624] Epoch[47] Batch[0] avg_epoch_loss=2.485952\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.4859521389\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:36 INFO 139961471010624] Epoch[47] Batch[5] avg_epoch_loss=2.590445\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.59044488271\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:36 INFO 139961471010624] Epoch[47] Batch [5]#011Speed: 60.82 samples/sec#011loss=2.590445\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] Epoch[47] Batch[10] avg_epoch_loss=2.586902\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=2.58265137672\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] Epoch[47] Batch [10]#011Speed: 60.15 samples/sec#011loss=2.582651\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16038.273811340332, \"sum\": 16038.273811340332, \"min\": 16038.273811340332}}, \"EndTime\": 1587680921.827014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680905.788416}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.5854018173 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.58690237999\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:41 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:47 INFO 139961471010624] Epoch[48] Batch[0] avg_epoch_loss=2.556493\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.55649256706\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:52 INFO 139961471010624] Epoch[48] Batch[5] avg_epoch_loss=2.630568\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.63056842486\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:52 INFO 139961471010624] Epoch[48] Batch [5]#011Speed: 61.22 samples/sec#011loss=2.630568\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:56 INFO 139961471010624] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14798.428058624268, \"sum\": 14798.428058624268, \"min\": 14798.428058624268}}, \"EndTime\": 1587680936.625768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680921.827073}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:56 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.4093972816 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:56 INFO 139961471010624] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.70764882565\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:28:56 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:01 INFO 139961471010624] Epoch[49] Batch[0] avg_epoch_loss=2.551109\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.55110883713\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:07 INFO 139961471010624] Epoch[49] Batch[5] avg_epoch_loss=2.569442\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.56944231192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:07 INFO 139961471010624] Epoch[49] Batch [5]#011Speed: 61.12 samples/sec#011loss=2.569442\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:10 INFO 139961471010624] processed a total of 564 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 13772.294998168945, \"sum\": 13772.294998168945, \"min\": 13772.294998168945}}, \"EndTime\": 1587680950.398508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680936.625847}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:10 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9514648995 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:10 INFO 139961471010624] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.6153854794\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:10 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:29:15 INFO 139961471010624] Epoch[50] Batch[0] avg_epoch_loss=2.571643\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:15 INFO 139961471010624] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.57164263725\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:20 INFO 139961471010624] Epoch[50] Batch[5] avg_epoch_loss=2.649390\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.64938950539\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:20 INFO 139961471010624] Epoch[50] Batch [5]#011Speed: 61.82 samples/sec#011loss=2.649390\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] Epoch[50] Batch[10] avg_epoch_loss=2.657359\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=2.66692142487\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] Epoch[50] Batch [10]#011Speed: 60.88 samples/sec#011loss=2.666921\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15739.703893661499, \"sum\": 15739.703893661499, \"min\": 15739.703893661499}}, \"EndTime\": 1587680966.138756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680950.398582}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.440172619 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.6573585597\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:26 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:31 INFO 139961471010624] Epoch[51] Batch[0] avg_epoch_loss=2.523352\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.52335238457\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:36 INFO 139961471010624] Epoch[51] Batch[5] avg_epoch_loss=2.613454\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.61345354716\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:36 INFO 139961471010624] Epoch[51] Batch [5]#011Speed: 61.44 samples/sec#011loss=2.613454\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:40 INFO 139961471010624] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14790.816068649292, \"sum\": 14790.816068649292, \"min\": 14790.816068649292}}, \"EndTime\": 1587680980.930033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680966.138823}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:40 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.931683657 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:40 INFO 139961471010624] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.63578047752\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:40 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:46 INFO 139961471010624] Epoch[52] Batch[0] avg_epoch_loss=2.549259\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.54925894737\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:51 INFO 139961471010624] Epoch[52] Batch[5] avg_epoch_loss=2.556383\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.55638289452\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:51 INFO 139961471010624] Epoch[52] Batch [5]#011Speed: 61.25 samples/sec#011loss=2.556383\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] Epoch[52] Batch[10] avg_epoch_loss=2.574921\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=2.59716730118\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] Epoch[52] Batch [10]#011Speed: 60.71 samples/sec#011loss=2.597167\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15847.5661277771, \"sum\": 15847.5661277771, \"min\": 15847.5661277771}}, \"EndTime\": 1587680996.778058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680980.930095}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.602648946 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.57492126118\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:29:56 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:02 INFO 139961471010624] Epoch[53] Batch[0] avg_epoch_loss=2.631507\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.63150668144\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:07 INFO 139961471010624] Epoch[53] Batch[5] avg_epoch_loss=2.633436\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.63343560696\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:07 INFO 139961471010624] Epoch[53] Batch [5]#011Speed: 61.66 samples/sec#011loss=2.633436\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:11 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14777.45795249939, \"sum\": 14777.45795249939, \"min\": 14777.45795249939}}, \"EndTime\": 1587681011.555962, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587680996.778124}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:11 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3615012555 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:11 INFO 139961471010624] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.59311990738\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:11 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:16 INFO 139961471010624] Epoch[54] Batch[0] avg_epoch_loss=2.551047\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.55104660988\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:22 INFO 139961471010624] Epoch[54] Batch[5] avg_epoch_loss=2.542124\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.54212391376\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:22 INFO 139961471010624] Epoch[54] Batch [5]#011Speed: 61.85 samples/sec#011loss=2.542124\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:26 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14637.194871902466, \"sum\": 14637.194871902466, \"min\": 14637.194871902466}}, \"EndTime\": 1587681026.193764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681011.556037}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:26 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.4941266528 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:26 INFO 139961471010624] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.57892177105\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:26 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:31 INFO 139961471010624] Epoch[55] Batch[0] avg_epoch_loss=2.581843\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.58184313774\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:36 INFO 139961471010624] Epoch[55] Batch[5] avg_epoch_loss=2.630418\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.630417943\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:36 INFO 139961471010624] Epoch[55] Batch [5]#011Speed: 61.33 samples/sec#011loss=2.630418\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] Epoch[55] Batch[10] avg_epoch_loss=2.690590\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.76279549599\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] Epoch[55] Batch [10]#011Speed: 60.51 samples/sec#011loss=2.762795\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15778.72896194458, \"sum\": 15778.72896194458, \"min\": 15778.72896194458}}, \"EndTime\": 1587681041.973031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681026.193857}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.2578273417 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.69058955799\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:41 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:30:47 INFO 139961471010624] Epoch[56] Batch[0] avg_epoch_loss=2.640340\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.64033961296\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:52 INFO 139961471010624] Epoch[56] Batch[5] avg_epoch_loss=2.608908\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.60890809695\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:52 INFO 139961471010624] Epoch[56] Batch [5]#011Speed: 61.93 samples/sec#011loss=2.608908\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:56 INFO 139961471010624] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14615.293979644775, \"sum\": 14615.293979644775, \"min\": 14615.293979644775}}, \"EndTime\": 1587681056.588921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681041.973091}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:56 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.8051875651 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:56 INFO 139961471010624] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.60911505222\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:30:56 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:01 INFO 139961471010624] Epoch[57] Batch[0] avg_epoch_loss=2.498074\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.49807357788\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:07 INFO 139961471010624] Epoch[57] Batch[5] avg_epoch_loss=2.563560\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.56355996927\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:07 INFO 139961471010624] Epoch[57] Batch [5]#011Speed: 61.50 samples/sec#011loss=2.563560\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14762.564897537231, \"sum\": 14762.564897537231, \"min\": 14762.564897537231}}, \"EndTime\": 1587681071.35187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681056.588988}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.2848912736 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.55121860504\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:11 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_bbb8c78a-2fdd-4fa5-a66e-9216e7c5a2e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 207.45205879211426, \"sum\": 207.45205879211426, \"min\": 207.45205879211426}}, \"EndTime\": 1587681071.5599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681071.351935}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:16 INFO 139961471010624] Epoch[58] Batch[0] avg_epoch_loss=2.630234\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.63023424149\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:22 INFO 139961471010624] Epoch[58] Batch[5] avg_epoch_loss=2.630932\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.63093229135\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:22 INFO 139961471010624] Epoch[58] Batch [5]#011Speed: 61.71 samples/sec#011loss=2.630932\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] Epoch[58] Batch[10] avg_epoch_loss=2.600721\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=2.56446671486\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] Epoch[58] Batch [10]#011Speed: 60.70 samples/sec#011loss=2.564467\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15796.207904815674, \"sum\": 15796.207904815674, \"min\": 15796.207904815674}}, \"EndTime\": 1587681087.356223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681071.559957}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.427869236 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.60072066567\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:27 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:32 INFO 139961471010624] Epoch[59] Batch[0] avg_epoch_loss=2.563307\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.56330657005\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:37 INFO 139961471010624] Epoch[59] Batch[5] avg_epoch_loss=2.554836\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.55483579636\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:37 INFO 139961471010624] Epoch[59] Batch [5]#011Speed: 61.91 samples/sec#011loss=2.554836\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] Epoch[59] Batch[10] avg_epoch_loss=2.581535\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=2.61357393265\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] Epoch[59] Batch [10]#011Speed: 60.46 samples/sec#011loss=2.613574\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15817.808151245117, \"sum\": 15817.808151245117, \"min\": 15817.808151245117}}, \"EndTime\": 1587681103.174496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681087.356289}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3570791846 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.58153494922\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:43 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:48 INFO 139961471010624] Epoch[60] Batch[0] avg_epoch_loss=2.516312\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.51631236076\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:53 INFO 139961471010624] Epoch[60] Batch[5] avg_epoch_loss=2.618280\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.6182804505\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:53 INFO 139961471010624] Epoch[60] Batch [5]#011Speed: 61.60 samples/sec#011loss=2.618280\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:57 INFO 139961471010624] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14734.457015991211, \"sum\": 14734.457015991211, \"min\": 14734.457015991211}}, \"EndTime\": 1587681117.909295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681103.174558}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:57 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.7386347077 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:57 INFO 139961471010624] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.59158287048\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:31:57 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:03 INFO 139961471010624] Epoch[61] Batch[0] avg_epoch_loss=2.748651\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.74865055084\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:32:08 INFO 139961471010624] Epoch[61] Batch[5] avg_epoch_loss=2.612033\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.61203320821\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:08 INFO 139961471010624] Epoch[61] Batch [5]#011Speed: 61.47 samples/sec#011loss=2.612033\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:12 INFO 139961471010624] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14738.193988800049, \"sum\": 14738.193988800049, \"min\": 14738.193988800049}}, \"EndTime\": 1587681132.648081, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681117.909355}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:12 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0664793342 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:12 INFO 139961471010624] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.62130770683\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:12 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:17 INFO 139961471010624] Epoch[62] Batch[0] avg_epoch_loss=2.583932\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.58393216133\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:23 INFO 139961471010624] Epoch[62] Batch[5] avg_epoch_loss=2.606941\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.60694130262\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:23 INFO 139961471010624] Epoch[62] Batch [5]#011Speed: 61.97 samples/sec#011loss=2.606941\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:27 INFO 139961471010624] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14641.589879989624, \"sum\": 14641.589879989624, \"min\": 14641.589879989624}}, \"EndTime\": 1587681147.291629, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681132.648309}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:27 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.3203727173 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:27 INFO 139961471010624] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.58245239258\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:27 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:32 INFO 139961471010624] Epoch[63] Batch[0] avg_epoch_loss=2.549796\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.54979634285\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:37 INFO 139961471010624] Epoch[63] Batch[5] avg_epoch_loss=2.531415\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.53141482671\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:37 INFO 139961471010624] Epoch[63] Batch [5]#011Speed: 61.49 samples/sec#011loss=2.531415\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:42 INFO 139961471010624] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14741.097927093506, \"sum\": 14741.097927093506, \"min\": 14741.097927093506}}, \"EndTime\": 1587681162.033196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681147.291694}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:42 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.5338679882 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:42 INFO 139961471010624] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.57599487305\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:42 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:47 INFO 139961471010624] Epoch[64] Batch[0] avg_epoch_loss=2.554661\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.5546605587\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:52 INFO 139961471010624] Epoch[64] Batch[5] avg_epoch_loss=2.550144\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.55014355977\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:52 INFO 139961471010624] Epoch[64] Batch [5]#011Speed: 61.92 samples/sec#011loss=2.550144\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] Epoch[64] Batch[10] avg_epoch_loss=2.505276\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=2.45143456459\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] Epoch[64] Batch [10]#011Speed: 61.22 samples/sec#011loss=2.451435\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15698.79698753357, \"sum\": 15698.79698753357, \"min\": 15698.79698753357}}, \"EndTime\": 1587681177.73261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681162.03326}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.4679107431 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.50527583469\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:32:57 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_3188f2e1-5807-44a6-8d80-58cca45a9355-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 219.01416778564453, \"sum\": 219.01416778564453, \"min\": 219.01416778564453}}, \"EndTime\": 1587681177.951996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681177.73267}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:03 INFO 139961471010624] Epoch[65] Batch[0] avg_epoch_loss=2.487151\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.48715114594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:08 INFO 139961471010624] Epoch[65] Batch[5] avg_epoch_loss=2.528394\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.52839422226\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:08 INFO 139961471010624] Epoch[65] Batch [5]#011Speed: 61.47 samples/sec#011loss=2.528394\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] Epoch[65] Batch[10] avg_epoch_loss=2.491900\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=2.44810724258\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] Epoch[65] Batch [10]#011Speed: 60.65 samples/sec#011loss=2.448107\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15861.44208908081, \"sum\": 15861.44208908081, \"min\": 15861.44208908081}}, \"EndTime\": 1587681193.813568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681177.952061}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.6730866622 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.49190014059\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:13 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:14 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_075a51a1-4cd5-441c-ac66-07716b4f8cd8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 208.47702026367188, \"sum\": 208.47702026367188, \"min\": 208.47702026367188}}, \"EndTime\": 1587681194.02264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681193.813647}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:33:19 INFO 139961471010624] Epoch[66] Batch[0] avg_epoch_loss=2.580717\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:19 INFO 139961471010624] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.58071660995\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:24 INFO 139961471010624] Epoch[66] Batch[5] avg_epoch_loss=2.566659\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:24 INFO 139961471010624] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.5666590929\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:24 INFO 139961471010624] Epoch[66] Batch [5]#011Speed: 61.88 samples/sec#011loss=2.566659\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:28 INFO 139961471010624] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14680.252075195312, \"sum\": 14680.252075195312, \"min\": 14680.252075195312}}, \"EndTime\": 1587681208.703015, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681194.0227}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:28 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.1869782394 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:28 INFO 139961471010624] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.56845023632\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:28 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:33 INFO 139961471010624] Epoch[67] Batch[0] avg_epoch_loss=2.566452\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.56645226479\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:39 INFO 139961471010624] Epoch[67] Batch[5] avg_epoch_loss=2.613715\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.61371501287\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:39 INFO 139961471010624] Epoch[67] Batch [5]#011Speed: 61.46 samples/sec#011loss=2.613715\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:43 INFO 139961471010624] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14705.620050430298, \"sum\": 14705.620050430298, \"min\": 14705.620050430298}}, \"EndTime\": 1587681223.40929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681208.703079}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:43 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9564830748 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:43 INFO 139961471010624] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.56554014683\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:43 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:48 INFO 139961471010624] Epoch[68] Batch[0] avg_epoch_loss=2.563310\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.56331038475\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:53 INFO 139961471010624] Epoch[68] Batch[5] avg_epoch_loss=2.557375\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.55737463633\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:53 INFO 139961471010624] Epoch[68] Batch [5]#011Speed: 61.57 samples/sec#011loss=2.557375\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:58 INFO 139961471010624] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14592.382907867432, \"sum\": 14592.382907867432, \"min\": 14592.382907867432}}, \"EndTime\": 1587681238.002282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681223.409351}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:58 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=39.6778597677 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:58 INFO 139961471010624] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.50566003323\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:33:58 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:03 INFO 139961471010624] Epoch[69] Batch[0] avg_epoch_loss=2.557327\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.55732655525\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:08 INFO 139961471010624] Epoch[69] Batch[5] avg_epoch_loss=2.540436\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.54043610891\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:08 INFO 139961471010624] Epoch[69] Batch [5]#011Speed: 61.74 samples/sec#011loss=2.540436\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:12 INFO 139961471010624] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14765.756130218506, \"sum\": 14765.756130218506, \"min\": 14765.756130218506}}, \"EndTime\": 1587681252.768629, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681238.002361}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:12 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=39.8893010705 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:12 INFO 139961471010624] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.56695754528\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:12 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:18 INFO 139961471010624] Epoch[70] Batch[0] avg_epoch_loss=2.566448\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.56644797325\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:23 INFO 139961471010624] Epoch[70] Batch[5] avg_epoch_loss=2.553075\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.55307467779\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:23 INFO 139961471010624] Epoch[70] Batch [5]#011Speed: 61.79 samples/sec#011loss=2.553075\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:27 INFO 139961471010624] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14657.166004180908, \"sum\": 14657.166004180908, \"min\": 14657.166004180908}}, \"EndTime\": 1587681267.426194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681252.768706}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:27 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.186710138 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:27 INFO 139961471010624] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:27 INFO 139961471010624] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.56742680073\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:27 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:32 INFO 139961471010624] Epoch[71] Batch[0] avg_epoch_loss=2.436841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.43684077263\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:37 INFO 139961471010624] Epoch[71] Batch[5] avg_epoch_loss=2.551930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.55192959309\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:37 INFO 139961471010624] Epoch[71] Batch [5]#011Speed: 61.86 samples/sec#011loss=2.551930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] Epoch[71] Batch[10] avg_epoch_loss=2.634987\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=2.73465595245\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] Epoch[71] Batch [10]#011Speed: 60.88 samples/sec#011loss=2.734656\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15736.22179031372, \"sum\": 15736.22179031372, \"min\": 15736.22179031372}}, \"EndTime\": 1587681283.162839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681267.426286}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9880135647 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.63498702916\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:43 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:34:48 INFO 139961471010624] Epoch[72] Batch[0] avg_epoch_loss=2.656508\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.65650844574\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:53 INFO 139961471010624] Epoch[72] Batch[5] avg_epoch_loss=2.557988\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.55798840523\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:53 INFO 139961471010624] Epoch[72] Batch [5]#011Speed: 61.45 samples/sec#011loss=2.557988\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:57 INFO 139961471010624] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14669.08597946167, \"sum\": 14669.08597946167, \"min\": 14669.08597946167}}, \"EndTime\": 1587681297.832421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681283.162891}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:57 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.6975299309 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:57 INFO 139961471010624] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.60258834362\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:34:57 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:03 INFO 139961471010624] Epoch[73] Batch[0] avg_epoch_loss=2.696167\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.69616675377\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:08 INFO 139961471010624] Epoch[73] Batch[5] avg_epoch_loss=2.581158\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.58115752538\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:08 INFO 139961471010624] Epoch[73] Batch [5]#011Speed: 61.92 samples/sec#011loss=2.581158\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] Epoch[73] Batch[10] avg_epoch_loss=2.564267\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.54399881363\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] Epoch[73] Batch [10]#011Speed: 60.40 samples/sec#011loss=2.543999\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15817.966938018799, \"sum\": 15817.966938018799, \"min\": 15817.966938018799}}, \"EndTime\": 1587681313.650899, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681297.832495}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.6842499498 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.56426720186\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:13 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:18 INFO 139961471010624] Epoch[74] Batch[0] avg_epoch_loss=2.637413\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.63741254807\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:24 INFO 139961471010624] Epoch[74] Batch[5] avg_epoch_loss=2.610036\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:24 INFO 139961471010624] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.61003649235\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:24 INFO 139961471010624] Epoch[74] Batch [5]#011Speed: 62.31 samples/sec#011loss=2.610036\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] Epoch[74] Batch[10] avg_epoch_loss=2.609093\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=2.60796189308\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] Epoch[74] Batch [10]#011Speed: 61.69 samples/sec#011loss=2.607962\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15671.033143997192, \"sum\": 15671.033143997192, \"min\": 15671.033143997192}}, \"EndTime\": 1587681329.322282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681313.650961}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.7747770399 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.60909349268\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:29 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:34 INFO 139961471010624] Epoch[75] Batch[0] avg_epoch_loss=2.558083\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.55808329582\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:39 INFO 139961471010624] Epoch[75] Batch[5] avg_epoch_loss=2.541916\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.54191644986\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:39 INFO 139961471010624] Epoch[75] Batch [5]#011Speed: 62.27 samples/sec#011loss=2.541916\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:43 INFO 139961471010624] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14640.348196029663, \"sum\": 14640.348196029663, \"min\": 14640.348196029663}}, \"EndTime\": 1587681343.963042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681329.322345}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:43 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.5779096792 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:43 INFO 139961471010624] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.5486325264\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:43 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:49 INFO 139961471010624] Epoch[76] Batch[0] avg_epoch_loss=2.456630\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.45662999153\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:54 INFO 139961471010624] Epoch[76] Batch[5] avg_epoch_loss=2.526419\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:54 INFO 139961471010624] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.52641948064\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:54 INFO 139961471010624] Epoch[76] Batch [5]#011Speed: 62.08 samples/sec#011loss=2.526419\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] Epoch[76] Batch[10] avg_epoch_loss=2.507983\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=2.48585925102\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] Epoch[76] Batch [10]#011Speed: 61.44 samples/sec#011loss=2.485859\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15659.807920455933, \"sum\": 15659.807920455933, \"min\": 15659.807920455933}}, \"EndTime\": 1587681359.623348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681343.963107}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.188001982 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.50798301263\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:35:59 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:05 INFO 139961471010624] Epoch[77] Batch[0] avg_epoch_loss=2.460914\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.46091365814\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:10 INFO 139961471010624] Epoch[77] Batch[5] avg_epoch_loss=2.530335\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.53033522765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:10 INFO 139961471010624] Epoch[77] Batch [5]#011Speed: 61.52 samples/sec#011loss=2.530335\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] Epoch[77] Batch[10] avg_epoch_loss=2.525461\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=2.5196120739\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] Epoch[77] Batch [10]#011Speed: 60.05 samples/sec#011loss=2.519612\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15934.53598022461, \"sum\": 15934.53598022461, \"min\": 15934.53598022461}}, \"EndTime\": 1587681375.558316, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681359.623409}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.8624784439 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.52546106685\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:15 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:36:20 INFO 139961471010624] Epoch[78] Batch[0] avg_epoch_loss=2.512269\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.51226949692\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:26 INFO 139961471010624] Epoch[78] Batch[5] avg_epoch_loss=2.513519\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.51351877054\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:26 INFO 139961471010624] Epoch[78] Batch [5]#011Speed: 61.94 samples/sec#011loss=2.513519\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] Epoch[78] Batch[10] avg_epoch_loss=2.500819\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=2.48558020592\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] Epoch[78] Batch [10]#011Speed: 61.07 samples/sec#011loss=2.485580\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15767.267942428589, \"sum\": 15767.267942428589, \"min\": 15767.267942428589}}, \"EndTime\": 1587681391.325935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681375.55838}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.0002207215 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.50081942298\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:31 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:36 INFO 139961471010624] Epoch[79] Batch[0] avg_epoch_loss=2.542718\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.54271817207\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:41 INFO 139961471010624] Epoch[79] Batch[5] avg_epoch_loss=2.507256\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.50725587209\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:41 INFO 139961471010624] Epoch[79] Batch [5]#011Speed: 61.99 samples/sec#011loss=2.507256\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] Epoch[79] Batch[10] avg_epoch_loss=2.492995\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=2.47588186264\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] Epoch[79] Batch [10]#011Speed: 60.64 samples/sec#011loss=2.475882\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15728.381872177124, \"sum\": 15728.381872177124, \"min\": 15728.381872177124}}, \"EndTime\": 1587681407.05481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681391.325997}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9448521879 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.4929949587\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:47 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:52 INFO 139961471010624] Epoch[80] Batch[0] avg_epoch_loss=2.584282\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.58428215981\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:57 INFO 139961471010624] Epoch[80] Batch[5] avg_epoch_loss=2.552793\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.55279318492\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:36:57 INFO 139961471010624] Epoch[80] Batch [5]#011Speed: 61.74 samples/sec#011loss=2.552793\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:01 INFO 139961471010624] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14701.02596282959, \"sum\": 14701.02596282959, \"min\": 14701.02596282959}}, \"EndTime\": 1587681421.75624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681407.054868}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:01 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3777328255 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:01 INFO 139961471010624] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.58104019165\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:01 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:07 INFO 139961471010624] Epoch[81] Batch[0] avg_epoch_loss=2.593697\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.59369683266\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:12 INFO 139961471010624] Epoch[81] Batch[5] avg_epoch_loss=2.507398\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.50739753246\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:12 INFO 139961471010624] Epoch[81] Batch [5]#011Speed: 62.11 samples/sec#011loss=2.507398\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] Epoch[81] Batch[10] avg_epoch_loss=2.509679\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=2.51241612434\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] Epoch[81] Batch [10]#011Speed: 60.82 samples/sec#011loss=2.512416\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15697.37696647644, \"sum\": 15697.37696647644, \"min\": 15697.37696647644}}, \"EndTime\": 1587681437.45416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681421.756303}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.3442447418 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.50967871059\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:17 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:22 INFO 139961471010624] Epoch[82] Batch[0] avg_epoch_loss=2.456715\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.45671510696\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:28 INFO 139961471010624] Epoch[82] Batch[5] avg_epoch_loss=2.544094\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.54409412543\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:28 INFO 139961471010624] Epoch[82] Batch [5]#011Speed: 61.11 samples/sec#011loss=2.544094\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] Epoch[82] Batch[10] avg_epoch_loss=2.563859\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=2.58757696152\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] Epoch[82] Batch [10]#011Speed: 60.91 samples/sec#011loss=2.587577\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15841.341018676758, \"sum\": 15841.341018676758, \"min\": 15841.341018676758}}, \"EndTime\": 1587681453.295868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681437.454221}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0416541918 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.56385905092\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:33 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:38 INFO 139961471010624] Epoch[83] Batch[0] avg_epoch_loss=2.382104\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.38210391998\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:37:43 INFO 139961471010624] Epoch[83] Batch[5] avg_epoch_loss=2.473511\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.47351090113\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:43 INFO 139961471010624] Epoch[83] Batch [5]#011Speed: 60.87 samples/sec#011loss=2.473511\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:47 INFO 139961471010624] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14681.957006454468, \"sum\": 14681.957006454468, \"min\": 14681.957006454468}}, \"EndTime\": 1587681467.978164, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681453.29593}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:47 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9559823308 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:47 INFO 139961471010624] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.50325598717\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:47 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:53 INFO 139961471010624] Epoch[84] Batch[0] avg_epoch_loss=2.501845\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.50184512138\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:58 INFO 139961471010624] Epoch[84] Batch[5] avg_epoch_loss=2.527537\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.52753698826\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:37:58 INFO 139961471010624] Epoch[84] Batch [5]#011Speed: 61.79 samples/sec#011loss=2.527537\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:02 INFO 139961471010624] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14651.468992233276, \"sum\": 14651.468992233276, \"min\": 14651.468992233276}}, \"EndTime\": 1587681482.63022, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681467.978229}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:02 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.681304762 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:02 INFO 139961471010624] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.50457024574\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:02 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:07 INFO 139961471010624] Epoch[85] Batch[0] avg_epoch_loss=2.598748\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.59874773026\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:13 INFO 139961471010624] Epoch[85] Batch[5] avg_epoch_loss=2.548187\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.54818713665\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:13 INFO 139961471010624] Epoch[85] Batch [5]#011Speed: 61.18 samples/sec#011loss=2.548187\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] Epoch[85] Batch[10] avg_epoch_loss=2.527325\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=2.50228991508\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] Epoch[85] Batch [10]#011Speed: 61.23 samples/sec#011loss=2.502290\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15773.902893066406, \"sum\": 15773.902893066406, \"min\": 15773.902893066406}}, \"EndTime\": 1587681498.404524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681482.630299}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0311990135 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.52732476321\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:18 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:23 INFO 139961471010624] Epoch[86] Batch[0] avg_epoch_loss=2.489352\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.48935174942\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:28 INFO 139961471010624] Epoch[86] Batch[5] avg_epoch_loss=2.613052\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.61305177212\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:28 INFO 139961471010624] Epoch[86] Batch [5]#011Speed: 61.60 samples/sec#011loss=2.613052\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:33 INFO 139961471010624] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14614.51005935669, \"sum\": 14614.51005935669, \"min\": 14614.51005935669}}, \"EndTime\": 1587681513.019444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681498.404584}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:33 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.7918154968 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:33 INFO 139961471010624] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.60066924095\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:33 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:38 INFO 139961471010624] Epoch[87] Batch[0] avg_epoch_loss=2.647504\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.64750385284\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:43 INFO 139961471010624] Epoch[87] Batch[5] avg_epoch_loss=2.562609\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.5626090765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:43 INFO 139961471010624] Epoch[87] Batch [5]#011Speed: 60.98 samples/sec#011loss=2.562609\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] Epoch[87] Batch[10] avg_epoch_loss=2.548413\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=2.53137869835\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] Epoch[87] Batch [10]#011Speed: 61.11 samples/sec#011loss=2.531379\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15757.358074188232, \"sum\": 15757.358074188232, \"min\": 15757.358074188232}}, \"EndTime\": 1587681528.777272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681513.019508}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.059932864 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.54841345007\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:48 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:54 INFO 139961471010624] Epoch[88] Batch[0] avg_epoch_loss=2.785778\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:54 INFO 139961471010624] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.78577804565\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:59 INFO 139961471010624] Epoch[88] Batch[5] avg_epoch_loss=2.666902\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:59 INFO 139961471010624] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.66690214475\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:38:59 INFO 139961471010624] Epoch[88] Batch [5]#011Speed: 61.69 samples/sec#011loss=2.666902\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:03 INFO 139961471010624] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14681.798934936523, \"sum\": 14681.798934936523, \"min\": 14681.798934936523}}, \"EndTime\": 1587681543.459529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681528.777336}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:03 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.8201262897 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:03 INFO 139961471010624] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.67636773586\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:03 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:39:08 INFO 139961471010624] Epoch[89] Batch[0] avg_epoch_loss=2.555187\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.55518651009\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:14 INFO 139961471010624] Epoch[89] Batch[5] avg_epoch_loss=2.588220\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:14 INFO 139961471010624] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.58821956317\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:14 INFO 139961471010624] Epoch[89] Batch [5]#011Speed: 60.78 samples/sec#011loss=2.588220\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:18 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14730.572938919067, \"sum\": 14730.572938919067, \"min\": 14730.572938919067}}, \"EndTime\": 1587681558.190585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681543.459619}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:18 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2248016208 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:18 INFO 139961471010624] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.58831105232\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:18 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:23 INFO 139961471010624] Epoch[90] Batch[0] avg_epoch_loss=2.663386\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.66338610649\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:28 INFO 139961471010624] Epoch[90] Batch[5] avg_epoch_loss=2.583500\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.58349959056\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:28 INFO 139961471010624] Epoch[90] Batch [5]#011Speed: 61.68 samples/sec#011loss=2.583500\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:32 INFO 139961471010624] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14657.694816589355, \"sum\": 14657.694816589355, \"min\": 14657.694816589355}}, \"EndTime\": 1587681572.84869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681558.19066}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:32 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3664410702 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:32 INFO 139961471010624] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.55975387096\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:32 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:38 INFO 139961471010624] Epoch[91] Batch[0] avg_epoch_loss=2.503449\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=2.50344944\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:43 INFO 139961471010624] Epoch[91] Batch[5] avg_epoch_loss=2.596585\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.59658479691\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:43 INFO 139961471010624] Epoch[91] Batch [5]#011Speed: 61.96 samples/sec#011loss=2.596585\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:47 INFO 139961471010624] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14689.491033554077, \"sum\": 14689.491033554077, \"min\": 14689.491033554077}}, \"EndTime\": 1587681587.538739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681572.84879}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:47 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.4321409041 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:47 INFO 139961471010624] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.58292043209\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:47 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:52 INFO 139961471010624] Epoch[92] Batch[0] avg_epoch_loss=2.614086\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.61408615112\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:58 INFO 139961471010624] Epoch[92] Batch[5] avg_epoch_loss=2.565620\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.56561974684\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:39:58 INFO 139961471010624] Epoch[92] Batch [5]#011Speed: 62.15 samples/sec#011loss=2.565620\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] Epoch[92] Batch[10] avg_epoch_loss=2.587335\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=2.6133934021\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] Epoch[92] Batch [10]#011Speed: 60.85 samples/sec#011loss=2.613393\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15722.028970718384, \"sum\": 15722.028970718384, \"min\": 15722.028970718384}}, \"EndTime\": 1587681603.261494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681587.538799}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.4243065491 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.58733504469\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:03 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:08 INFO 139961471010624] Epoch[93] Batch[0] avg_epoch_loss=2.524405\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.52440476418\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:13 INFO 139961471010624] Epoch[93] Batch[5] avg_epoch_loss=2.567055\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.56705534458\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:13 INFO 139961471010624] Epoch[93] Batch [5]#011Speed: 61.08 samples/sec#011loss=2.567055\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:17 INFO 139961471010624] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14691.048860549927, \"sum\": 14691.048860549927, \"min\": 14691.048860549927}}, \"EndTime\": 1587681617.952951, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681603.261554}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:17 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6786684524 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:17 INFO 139961471010624] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.55383605957\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:17 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:23 INFO 139961471010624] Epoch[94] Batch[0] avg_epoch_loss=2.460515\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.46051454544\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:28 INFO 139961471010624] Epoch[94] Batch[5] avg_epoch_loss=2.510878\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.51087812583\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:28 INFO 139961471010624] Epoch[94] Batch [5]#011Speed: 62.39 samples/sec#011loss=2.510878\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:32 INFO 139961471010624] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14545.897006988525, \"sum\": 14545.897006988525, \"min\": 14545.897006988525}}, \"EndTime\": 1587681632.499317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681617.953051}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:32 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3484028999 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:32 INFO 139961471010624] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.5684889555\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:32 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:40:37 INFO 139961471010624] Epoch[95] Batch[0] avg_epoch_loss=2.542497\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=2.54249739647\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:42 INFO 139961471010624] Epoch[95] Batch[5] avg_epoch_loss=2.551905\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.55190459887\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:42 INFO 139961471010624] Epoch[95] Batch [5]#011Speed: 62.39 samples/sec#011loss=2.551905\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] Epoch[95] Batch[10] avg_epoch_loss=2.478803\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.39108073711\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] Epoch[95] Batch [10]#011Speed: 61.37 samples/sec#011loss=2.391081\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15589.767932891846, \"sum\": 15589.767932891846, \"min\": 15589.767932891846}}, \"EndTime\": 1587681648.089598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681632.49939}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.2447629559 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.47880284353\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:48 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_5efd525c-47ba-475e-a4a6-00bdded3304b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 210.71100234985352, \"sum\": 210.71100234985352, \"min\": 210.71100234985352}}, \"EndTime\": 1587681648.300725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681648.089658}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:53 INFO 139961471010624] Epoch[96] Batch[0] avg_epoch_loss=2.556146\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=2.55614566803\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:58 INFO 139961471010624] Epoch[96] Batch[5] avg_epoch_loss=2.606393\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.60639337699\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:40:58 INFO 139961471010624] Epoch[96] Batch [5]#011Speed: 61.86 samples/sec#011loss=2.606393\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:02 INFO 139961471010624] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14630.32579421997, \"sum\": 14630.32579421997, \"min\": 14630.32579421997}}, \"EndTime\": 1587681662.931169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681648.300785}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:02 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3774519434 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:02 INFO 139961471010624] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.57000916004\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:02 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:08 INFO 139961471010624] Epoch[97] Batch[0] avg_epoch_loss=2.529982\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.52998232841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:13 INFO 139961471010624] Epoch[97] Batch[5] avg_epoch_loss=2.547640\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.54763996601\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:13 INFO 139961471010624] Epoch[97] Batch [5]#011Speed: 61.70 samples/sec#011loss=2.547640\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:17 INFO 139961471010624] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14718.201160430908, \"sum\": 14718.201160430908, \"min\": 14718.201160430908}}, \"EndTime\": 1587681677.64983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681662.931234}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:17 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.2794748533 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:17 INFO 139961471010624] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.55057256222\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:17 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:22 INFO 139961471010624] Epoch[98] Batch[0] avg_epoch_loss=2.446643\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.44664311409\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:28 INFO 139961471010624] Epoch[98] Batch[5] avg_epoch_loss=2.554795\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.55479546388\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:28 INFO 139961471010624] Epoch[98] Batch [5]#011Speed: 62.02 samples/sec#011loss=2.554795\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] Epoch[98] Batch[10] avg_epoch_loss=2.572891\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=2.59460458755\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] Epoch[98] Batch [10]#011Speed: 61.21 samples/sec#011loss=2.594605\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15677.464962005615, \"sum\": 15677.464962005615, \"min\": 15677.464962005615}}, \"EndTime\": 1587681693.327796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681677.649892}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.1188364526 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.5728905201\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:33 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:38 INFO 139961471010624] Epoch[99] Batch[0] avg_epoch_loss=2.465192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:38 INFO 139961471010624] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.46519207954\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:43 INFO 139961471010624] Epoch[99] Batch[5] avg_epoch_loss=2.520777\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.52077702681\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:43 INFO 139961471010624] Epoch[99] Batch [5]#011Speed: 62.08 samples/sec#011loss=2.520777\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] Epoch[99] Batch[10] avg_epoch_loss=2.546140\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=2.57657461166\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] Epoch[99] Batch [10]#011Speed: 60.89 samples/sec#011loss=2.576575\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15749.403953552246, \"sum\": 15749.403953552246, \"min\": 15749.403953552246}}, \"EndTime\": 1587681709.077846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681693.327897}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9696046644 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.54613956538\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:49 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:41:54 INFO 139961471010624] Epoch[100] Batch[0] avg_epoch_loss=2.611029\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:54 INFO 139961471010624] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.61102938652\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:59 INFO 139961471010624] Epoch[100] Batch[5] avg_epoch_loss=2.508635\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:59 INFO 139961471010624] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.50863476594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:41:59 INFO 139961471010624] Epoch[100] Batch [5]#011Speed: 61.55 samples/sec#011loss=2.508635\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:03 INFO 139961471010624] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14699.090003967285, \"sum\": 14699.090003967285, \"min\": 14699.090003967285}}, \"EndTime\": 1587681723.777265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681709.077904}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:03 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6553302712 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:03 INFO 139961471010624] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.51988902092\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:03 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:09 INFO 139961471010624] Epoch[101] Batch[0] avg_epoch_loss=2.510098\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:09 INFO 139961471010624] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.51009774208\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:14 INFO 139961471010624] Epoch[101] Batch[5] avg_epoch_loss=2.510492\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:14 INFO 139961471010624] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=2.51049232483\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:14 INFO 139961471010624] Epoch[101] Batch [5]#011Speed: 61.41 samples/sec#011loss=2.510492\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] Epoch[101] Batch[10] avg_epoch_loss=2.464808\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=2.40998644829\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] Epoch[101] Batch [10]#011Speed: 60.64 samples/sec#011loss=2.409986\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15797.693967819214, \"sum\": 15797.693967819214, \"min\": 15797.693967819214}}, \"EndTime\": 1587681739.575645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681723.777361}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9045688206 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] #quality_metric: host=algo-1, epoch=101, train loss <loss>=2.46480783549\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:19 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_07e9620a-daa7-49b1-8224-86e4bfdcc44b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 209.824800491333, \"sum\": 209.824800491333, \"min\": 209.824800491333}}, \"EndTime\": 1587681739.786011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681739.575716}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:25 INFO 139961471010624] Epoch[102] Batch[0] avg_epoch_loss=2.344478\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.34447813034\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:30 INFO 139961471010624] Epoch[102] Batch[5] avg_epoch_loss=2.478537\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:30 INFO 139961471010624] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.47853680452\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:30 INFO 139961471010624] Epoch[102] Batch [5]#011Speed: 61.96 samples/sec#011loss=2.478537\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] Epoch[102] Batch[10] avg_epoch_loss=2.505407\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=2.5376502037\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] Epoch[102] Batch [10]#011Speed: 60.47 samples/sec#011loss=2.537650\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15769.536972045898, \"sum\": 15769.536972045898, \"min\": 15769.536972045898}}, \"EndTime\": 1587681755.555668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681739.786075}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.5989658222 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] #quality_metric: host=algo-1, epoch=102, train loss <loss>=2.50540653142\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:35 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:40 INFO 139961471010624] Epoch[103] Batch[0] avg_epoch_loss=2.519262\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.51926207542\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:46 INFO 139961471010624] Epoch[103] Batch[5] avg_epoch_loss=2.516875\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.51687542597\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:46 INFO 139961471010624] Epoch[103] Batch [5]#011Speed: 61.01 samples/sec#011loss=2.516875\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] Epoch[103] Batch[10] avg_epoch_loss=2.407731\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=2.27675683498\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] Epoch[103] Batch [10]#011Speed: 61.61 samples/sec#011loss=2.276757\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15712.892055511475, \"sum\": 15712.892055511475, \"min\": 15712.892055511475}}, \"EndTime\": 1587681771.268876, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681755.555726}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.2397927071 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.40773061189\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:51 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/state_48550185-3c3a-418f-be82-6dc4445d9f56-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 217.14305877685547, \"sum\": 217.14305877685547, \"min\": 217.14305877685547}}, \"EndTime\": 1587681771.486449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681771.268931}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:56 INFO 139961471010624] Epoch[104] Batch[0] avg_epoch_loss=2.517300\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:42:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.51730012894\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:01 INFO 139961471010624] Epoch[104] Batch[5] avg_epoch_loss=2.510524\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.51052411397\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:01 INFO 139961471010624] Epoch[104] Batch [5]#011Speed: 61.64 samples/sec#011loss=2.510524\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:06 INFO 139961471010624] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14684.67402458191, \"sum\": 14684.67402458191, \"min\": 14684.67402458191}}, \"EndTime\": 1587681786.171241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681771.486507}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:06 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.8801217566 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:06 INFO 139961471010624] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.52454459667\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:06 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:43:11 INFO 139961471010624] Epoch[105] Batch[0] avg_epoch_loss=2.497335\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:11 INFO 139961471010624] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=2.49733495712\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:16 INFO 139961471010624] Epoch[105] Batch[5] avg_epoch_loss=2.506161\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.50616061687\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:16 INFO 139961471010624] Epoch[105] Batch [5]#011Speed: 61.51 samples/sec#011loss=2.506161\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:20 INFO 139961471010624] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14673.171043395996, \"sum\": 14673.171043395996, \"min\": 14673.171043395996}}, \"EndTime\": 1587681800.844917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681786.171305}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.4803851085 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.51822857857\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:20 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:26 INFO 139961471010624] Epoch[106] Batch[0] avg_epoch_loss=2.682033\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=2.68203258514\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:31 INFO 139961471010624] Epoch[106] Batch[5] avg_epoch_loss=2.587638\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=2.5876382192\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:31 INFO 139961471010624] Epoch[106] Batch [5]#011Speed: 61.93 samples/sec#011loss=2.587638\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] Epoch[106] Batch[10] avg_epoch_loss=2.506681\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=2.40953247547\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] Epoch[106] Batch [10]#011Speed: 61.08 samples/sec#011loss=2.409532\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15739.6559715271, \"sum\": 15739.6559715271, \"min\": 15739.6559715271}}, \"EndTime\": 1587681816.584983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681800.844995}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0591314622 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=106, train loss <loss>=2.50668106296\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:36 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:41 INFO 139961471010624] Epoch[107] Batch[0] avg_epoch_loss=2.628520\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=2.62852048874\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:47 INFO 139961471010624] Epoch[107] Batch[5] avg_epoch_loss=2.547841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.54784075419\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:47 INFO 139961471010624] Epoch[107] Batch [5]#011Speed: 61.07 samples/sec#011loss=2.547841\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:51 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14693.306922912598, \"sum\": 14693.306922912598, \"min\": 14693.306922912598}}, \"EndTime\": 1587681831.278622, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681816.585044}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:51 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3319254317 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:51 INFO 139961471010624] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=107, train loss <loss>=2.59137449265\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:51 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:56 INFO 139961471010624] Epoch[108] Batch[0] avg_epoch_loss=2.438025\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:43:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.43802475929\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:01 INFO 139961471010624] Epoch[108] Batch[5] avg_epoch_loss=2.506171\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.50617138545\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:01 INFO 139961471010624] Epoch[108] Batch [5]#011Speed: 61.65 samples/sec#011loss=2.506171\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] Epoch[108] Batch[10] avg_epoch_loss=2.494318\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=2.48009467125\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] Epoch[108] Batch [10]#011Speed: 61.16 samples/sec#011loss=2.480095\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15715.736150741577, \"sum\": 15715.736150741577, \"min\": 15715.736150741577}}, \"EndTime\": 1587681846.994739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681831.278688}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.8505418887 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.49431833354\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:06 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:12 INFO 139961471010624] Epoch[109] Batch[0] avg_epoch_loss=2.488925\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.48892521858\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:17 INFO 139961471010624] Epoch[109] Batch[5] avg_epoch_loss=2.499331\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=2.49933119615\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:17 INFO 139961471010624] Epoch[109] Batch [5]#011Speed: 61.32 samples/sec#011loss=2.499331\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:21 INFO 139961471010624] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14674.745082855225, \"sum\": 14674.745082855225, \"min\": 14674.745082855225}}, \"EndTime\": 1587681861.669818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681846.9948}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:21 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.4535800759 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:21 INFO 139961471010624] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:21 INFO 139961471010624] #quality_metric: host=algo-1, epoch=109, train loss <loss>=2.51221189499\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:21 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:26 INFO 139961471010624] Epoch[110] Batch[0] avg_epoch_loss=2.582860\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.58285999298\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:32 INFO 139961471010624] Epoch[110] Batch[5] avg_epoch_loss=2.533694\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:32 INFO 139961471010624] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.53369386991\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:32 INFO 139961471010624] Epoch[110] Batch [5]#011Speed: 62.02 samples/sec#011loss=2.533694\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] Epoch[110] Batch[10] avg_epoch_loss=2.563106\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=2.59839992523\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] Epoch[110] Batch [10]#011Speed: 61.37 samples/sec#011loss=2.598400\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15698.955059051514, \"sum\": 15698.955059051514, \"min\": 15698.955059051514}}, \"EndTime\": 1587681877.36943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681861.669889}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.6332275303 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.56310571324\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:37 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:42 INFO 139961471010624] Epoch[111] Batch[0] avg_epoch_loss=2.522187\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=2.52218651772\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:47 INFO 139961471010624] Epoch[111] Batch[5] avg_epoch_loss=2.554032\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.55403161049\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:47 INFO 139961471010624] Epoch[111] Batch [5]#011Speed: 60.73 samples/sec#011loss=2.554032\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:51 INFO 139961471010624] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 13695.839881896973, \"sum\": 13695.839881896973, \"min\": 13695.839881896973}}, \"EndTime\": 1587681891.065595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681877.369489}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:51 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9832658098 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:51 INFO 139961471010624] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=111, train loss <loss>=2.54007095761\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:51 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:56 INFO 139961471010624] Epoch[112] Batch[0] avg_epoch_loss=2.558214\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:44:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=2.55821442604\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:01 INFO 139961471010624] Epoch[112] Batch[5] avg_epoch_loss=2.512930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=2.51293027401\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:01 INFO 139961471010624] Epoch[112] Batch [5]#011Speed: 61.65 samples/sec#011loss=2.512930\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] Epoch[112] Batch[10] avg_epoch_loss=2.508892\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=2.50404586792\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] Epoch[112] Batch [10]#011Speed: 60.93 samples/sec#011loss=2.504046\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15731.931924819946, \"sum\": 15731.931924819946, \"min\": 15731.931924819946}}, \"EndTime\": 1587681906.79798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681891.065659}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2069081824 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] #quality_metric: host=algo-1, epoch=112, train loss <loss>=2.50889190761\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:06 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:12 INFO 139961471010624] Epoch[113] Batch[0] avg_epoch_loss=2.318338\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=2.31833791733\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:17 INFO 139961471010624] Epoch[113] Batch[5] avg_epoch_loss=2.501313\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=2.50131277243\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:17 INFO 139961471010624] Epoch[113] Batch [5]#011Speed: 61.29 samples/sec#011loss=2.501313\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:21 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14649.01089668274, \"sum\": 14649.01089668274, \"min\": 14649.01089668274}}, \"EndTime\": 1587681921.447382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681906.798041}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:21 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.7329629153 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:21 INFO 139961471010624] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:21 INFO 139961471010624] #quality_metric: host=algo-1, epoch=113, train loss <loss>=2.53022584915\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:21 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:26 INFO 139961471010624] Epoch[114] Batch[0] avg_epoch_loss=2.486083\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:26 INFO 139961471010624] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=2.48608326912\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:31 INFO 139961471010624] Epoch[114] Batch[5] avg_epoch_loss=2.520883\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=2.52088324229\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:31 INFO 139961471010624] Epoch[114] Batch [5]#011Speed: 62.32 samples/sec#011loss=2.520883\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] Epoch[114] Batch[10] avg_epoch_loss=2.437887\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=2.33829069138\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] Epoch[114] Batch [10]#011Speed: 61.80 samples/sec#011loss=2.338291\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15569.537878036499, \"sum\": 15569.537878036499, \"min\": 15569.537878036499}}, \"EndTime\": 1587681937.017625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681921.44745}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.4268118271 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=114, train loss <loss>=2.43788662824\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:37 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:42 INFO 139961471010624] Epoch[115] Batch[0] avg_epoch_loss=2.591753\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.59175300598\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:47 INFO 139961471010624] Epoch[115] Batch[5] avg_epoch_loss=2.510295\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:47 INFO 139961471010624] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=2.51029451688\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:47 INFO 139961471010624] Epoch[115] Batch [5]#011Speed: 61.78 samples/sec#011loss=2.510295\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] Epoch[115] Batch[10] avg_epoch_loss=2.532883\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=2.559988451\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] Epoch[115] Batch [10]#011Speed: 61.60 samples/sec#011loss=2.559988\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15646.674156188965, \"sum\": 15646.674156188965, \"min\": 15646.674156188965}}, \"EndTime\": 1587681952.66479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681937.017682}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.0947673095 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=115, train loss <loss>=2.53288266876\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:52 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:45:57 INFO 139961471010624] Epoch[116] Batch[0] avg_epoch_loss=2.591634\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:45:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=2.59163379669\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:03 INFO 139961471010624] Epoch[116] Batch[5] avg_epoch_loss=2.531063\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:03 INFO 139961471010624] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=2.53106300036\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:03 INFO 139961471010624] Epoch[116] Batch [5]#011Speed: 61.77 samples/sec#011loss=2.531063\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] Epoch[116] Batch[10] avg_epoch_loss=2.497310\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=2.45680689812\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] Epoch[116] Batch [10]#011Speed: 61.49 samples/sec#011loss=2.456807\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15694.2720413208, \"sum\": 15694.2720413208, \"min\": 15694.2720413208}}, \"EndTime\": 1587681968.359385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681952.66485}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.6709807438 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=116, train loss <loss>=2.49731022661\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:08 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:13 INFO 139961471010624] Epoch[117] Batch[0] avg_epoch_loss=2.553242\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=2.55324220657\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:18 INFO 139961471010624] Epoch[117] Batch[5] avg_epoch_loss=2.597870\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=2.59786979357\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:18 INFO 139961471010624] Epoch[117] Batch [5]#011Speed: 61.17 samples/sec#011loss=2.597870\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:23 INFO 139961471010624] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14731.055974960327, \"sum\": 14731.055974960327, \"min\": 14731.055974960327}}, \"EndTime\": 1587681983.090959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681968.359456}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:23 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.6125124248 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:23 INFO 139961471010624] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:23 INFO 139961471010624] #quality_metric: host=algo-1, epoch=117, train loss <loss>=2.51633057594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:23 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:28 INFO 139961471010624] Epoch[118] Batch[0] avg_epoch_loss=2.478714\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=2.47871446609\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:33 INFO 139961471010624] Epoch[118] Batch[5] avg_epoch_loss=2.522461\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=2.52246089776\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:33 INFO 139961471010624] Epoch[118] Batch [5]#011Speed: 61.66 samples/sec#011loss=2.522461\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:37 INFO 139961471010624] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14682.554006576538, \"sum\": 14682.554006576538, \"min\": 14682.554006576538}}, \"EndTime\": 1587681997.774004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681983.091021}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:37 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.8180496636 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:37 INFO 139961471010624] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=118, train loss <loss>=2.50702543259\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:37 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:43 INFO 139961471010624] Epoch[119] Batch[0] avg_epoch_loss=2.577991\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:43 INFO 139961471010624] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=2.57799100876\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:48 INFO 139961471010624] Epoch[119] Batch[5] avg_epoch_loss=2.522722\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=2.52272180716\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:48 INFO 139961471010624] Epoch[119] Batch [5]#011Speed: 61.08 samples/sec#011loss=2.522722\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] Epoch[119] Batch[10] avg_epoch_loss=2.511806\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=2.49870810509\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] Epoch[119] Batch [10]#011Speed: 61.25 samples/sec#011loss=2.498708\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15822.661876678467, \"sum\": 15822.661876678467, \"min\": 15822.661876678467}}, \"EndTime\": 1587682013.597175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587681997.77407}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.4816816492 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] #quality_metric: host=algo-1, epoch=119, train loss <loss>=2.51180648804\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:53 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:58 INFO 139961471010624] Epoch[120] Batch[0] avg_epoch_loss=2.495785\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:46:58 INFO 139961471010624] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=2.4957845211\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:04 INFO 139961471010624] Epoch[120] Batch[5] avg_epoch_loss=2.529141\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:04 INFO 139961471010624] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=2.52914051215\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:04 INFO 139961471010624] Epoch[120] Batch [5]#011Speed: 61.71 samples/sec#011loss=2.529141\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:08 INFO 139961471010624] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14645.832061767578, \"sum\": 14645.832061767578, \"min\": 14645.832061767578}}, \"EndTime\": 1587682028.243433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682013.597237}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:08 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.5371328518 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:08 INFO 139961471010624] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:08 INFO 139961471010624] #quality_metric: host=algo-1, epoch=120, train loss <loss>=2.52466435432\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:08 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:13 INFO 139961471010624] Epoch[121] Batch[0] avg_epoch_loss=2.533700\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:13 INFO 139961471010624] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=2.53369998932\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:18 INFO 139961471010624] Epoch[121] Batch[5] avg_epoch_loss=2.483294\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:18 INFO 139961471010624] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=2.48329416911\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:18 INFO 139961471010624] Epoch[121] Batch [5]#011Speed: 61.24 samples/sec#011loss=2.483294\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:22 INFO 139961471010624] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14644.511938095093, \"sum\": 14644.511938095093, \"min\": 14644.511938095093}}, \"EndTime\": 1587682042.888625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682028.243491}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:22 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.087518933 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:22 INFO 139961471010624] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:22 INFO 139961471010624] #quality_metric: host=algo-1, epoch=121, train loss <loss>=2.51929728985\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:22 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:47:28 INFO 139961471010624] Epoch[122] Batch[0] avg_epoch_loss=2.497370\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:28 INFO 139961471010624] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=2.49737024307\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:33 INFO 139961471010624] Epoch[122] Batch[5] avg_epoch_loss=2.482946\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:33 INFO 139961471010624] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=2.48294568062\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:33 INFO 139961471010624] Epoch[122] Batch [5]#011Speed: 61.48 samples/sec#011loss=2.482946\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:37 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14689.244985580444, \"sum\": 14689.244985580444, \"min\": 14689.244985580444}}, \"EndTime\": 1587682057.578372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682042.888689}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:37 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6158019134 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:37 INFO 139961471010624] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:37 INFO 139961471010624] #quality_metric: host=algo-1, epoch=122, train loss <loss>=2.48721334934\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:37 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:42 INFO 139961471010624] Epoch[123] Batch[0] avg_epoch_loss=2.498380\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:42 INFO 139961471010624] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=2.49838018417\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:48 INFO 139961471010624] Epoch[123] Batch[5] avg_epoch_loss=2.522740\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:48 INFO 139961471010624] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=2.52273964882\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:48 INFO 139961471010624] Epoch[123] Batch [5]#011Speed: 61.24 samples/sec#011loss=2.522740\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:52 INFO 139961471010624] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14641.294002532959, \"sum\": 14641.294002532959, \"min\": 14641.294002532959}}, \"EndTime\": 1587682072.220521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682057.57846}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:52 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.0042083723 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:52 INFO 139961471010624] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:52 INFO 139961471010624] #quality_metric: host=algo-1, epoch=123, train loss <loss>=2.52203192711\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:52 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:57 INFO 139961471010624] Epoch[124] Batch[0] avg_epoch_loss=2.514673\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:47:57 INFO 139961471010624] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=2.51467323303\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:02 INFO 139961471010624] Epoch[124] Batch[5] avg_epoch_loss=2.499765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:02 INFO 139961471010624] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=2.49976460139\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:02 INFO 139961471010624] Epoch[124] Batch [5]#011Speed: 60.82 samples/sec#011loss=2.499765\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:07 INFO 139961471010624] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14796.763896942139, \"sum\": 14796.763896942139, \"min\": 14796.763896942139}}, \"EndTime\": 1587682087.017823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682072.220585}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:07 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.1847182416 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:07 INFO 139961471010624] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:07 INFO 139961471010624] #quality_metric: host=algo-1, epoch=124, train loss <loss>=2.5133746624\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:07 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:12 INFO 139961471010624] Epoch[125] Batch[0] avg_epoch_loss=2.383787\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:12 INFO 139961471010624] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=2.38378691673\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:17 INFO 139961471010624] Epoch[125] Batch[5] avg_epoch_loss=2.462597\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:17 INFO 139961471010624] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=2.4625972112\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:17 INFO 139961471010624] Epoch[125] Batch [5]#011Speed: 61.48 samples/sec#011loss=2.462597\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:20 INFO 139961471010624] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 13592.617988586426, \"sum\": 13592.617988586426, \"min\": 13592.617988586426}}, \"EndTime\": 1587682100.611015, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682087.017929}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.3756497715 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=125, train loss <loss>=2.50034814411\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:20 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:25 INFO 139961471010624] Epoch[126] Batch[0] avg_epoch_loss=2.523924\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=2.52392363548\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:31 INFO 139961471010624] Epoch[126] Batch[5] avg_epoch_loss=2.514038\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=2.51403756936\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:31 INFO 139961471010624] Epoch[126] Batch [5]#011Speed: 61.91 samples/sec#011loss=2.514038\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] Epoch[126] Batch[10] avg_epoch_loss=2.546584\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=2.58563947678\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] Epoch[126] Batch [10]#011Speed: 61.06 samples/sec#011loss=2.585639\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15715.71397781372, \"sum\": 15715.71397781372, \"min\": 15715.71397781372}}, \"EndTime\": 1587682116.327187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682100.61108}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.7414244657 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=126, train loss <loss>=2.54658389091\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:36 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:41 INFO 139961471010624] Epoch[127] Batch[0] avg_epoch_loss=2.474722\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=2.47472238541\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:46 INFO 139961471010624] Epoch[127] Batch[5] avg_epoch_loss=2.511640\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=2.5116396745\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:46 INFO 139961471010624] Epoch[127] Batch [5]#011Speed: 61.69 samples/sec#011loss=2.511640\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:51 INFO 139961471010624] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14754.88018989563, \"sum\": 14754.88018989563, \"min\": 14754.88018989563}}, \"EndTime\": 1587682131.08251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682116.327247}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:51 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.1718616399 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:51 INFO 139961471010624] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:51 INFO 139961471010624] #quality_metric: host=algo-1, epoch=127, train loss <loss>=2.52299239635\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:51 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:56 INFO 139961471010624] Epoch[128] Batch[0] avg_epoch_loss=2.480710\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:48:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=2.4807100296\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:49:01 INFO 139961471010624] Epoch[128] Batch[5] avg_epoch_loss=2.487455\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=2.48745524883\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:01 INFO 139961471010624] Epoch[128] Batch [5]#011Speed: 61.94 samples/sec#011loss=2.487455\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:05 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14582.728147506714, \"sum\": 14582.728147506714, \"min\": 14582.728147506714}}, \"EndTime\": 1587682145.665682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682131.082578}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:05 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6529294366 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:05 INFO 139961471010624] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=128, train loss <loss>=2.50563058853\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:05 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:10 INFO 139961471010624] Epoch[129] Batch[0] avg_epoch_loss=2.608451\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=2.60845088959\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:16 INFO 139961471010624] Epoch[129] Batch[5] avg_epoch_loss=2.559925\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=2.55992460251\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:16 INFO 139961471010624] Epoch[129] Batch [5]#011Speed: 61.60 samples/sec#011loss=2.559925\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:20 INFO 139961471010624] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14742.25902557373, \"sum\": 14742.25902557373, \"min\": 14742.25902557373}}, \"EndTime\": 1587682160.408473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682145.665742}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.3443997152 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=129, train loss <loss>=2.54806623459\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:20 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:25 INFO 139961471010624] Epoch[130] Batch[0] avg_epoch_loss=2.521033\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=2.52103304863\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:30 INFO 139961471010624] Epoch[130] Batch[5] avg_epoch_loss=2.520479\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:30 INFO 139961471010624] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=2.52047900359\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:30 INFO 139961471010624] Epoch[130] Batch [5]#011Speed: 61.89 samples/sec#011loss=2.520479\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] Epoch[130] Batch[10] avg_epoch_loss=2.594188\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=2.68263821602\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] Epoch[130] Batch [10]#011Speed: 61.53 samples/sec#011loss=2.682638\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15697.700023651123, \"sum\": 15697.700023651123, \"min\": 15697.700023651123}}, \"EndTime\": 1587682176.106632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682160.408549}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.7893221828 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] #quality_metric: host=algo-1, epoch=130, train loss <loss>=2.59418773651\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:36 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:41 INFO 139961471010624] Epoch[131] Batch[0] avg_epoch_loss=2.594922\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:41 INFO 139961471010624] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=2.59492182732\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:46 INFO 139961471010624] Epoch[131] Batch[5] avg_epoch_loss=2.601115\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:46 INFO 139961471010624] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=2.6011150678\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:46 INFO 139961471010624] Epoch[131] Batch [5]#011Speed: 61.93 samples/sec#011loss=2.601115\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:50 INFO 139961471010624] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14679.975986480713, \"sum\": 14679.975986480713, \"min\": 14679.975986480713}}, \"EndTime\": 1587682190.787019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682176.106689}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:50 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6428306492 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:50 INFO 139961471010624] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:50 INFO 139961471010624] #quality_metric: host=algo-1, epoch=131, train loss <loss>=2.59584710598\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:50 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:56 INFO 139961471010624] Epoch[132] Batch[0] avg_epoch_loss=2.454083\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:49:56 INFO 139961471010624] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=2.45408296585\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:01 INFO 139961471010624] Epoch[132] Batch[5] avg_epoch_loss=2.499762\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:01 INFO 139961471010624] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=2.4997622172\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:01 INFO 139961471010624] Epoch[132] Batch [5]#011Speed: 61.85 samples/sec#011loss=2.499762\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:05 INFO 139961471010624] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14709.190130233765, \"sum\": 14709.190130233765, \"min\": 14709.190130233765}}, \"EndTime\": 1587682205.496716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682190.787086}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:05 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.2861842269 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:05 INFO 139961471010624] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=132, train loss <loss>=2.51155328751\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:05 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:10 INFO 139961471010624] Epoch[133] Batch[0] avg_epoch_loss=2.567271\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=2.56727147102\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:16 INFO 139961471010624] Epoch[133] Batch[5] avg_epoch_loss=2.484775\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=2.48477518559\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:16 INFO 139961471010624] Epoch[133] Batch [5]#011Speed: 61.28 samples/sec#011loss=2.484775\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:20 INFO 139961471010624] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14747.07007408142, \"sum\": 14747.07007408142, \"min\": 14747.07007408142}}, \"EndTime\": 1587682220.244319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682205.496787}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.8213691344 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=133, train loss <loss>=2.49278485775\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:20 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:50:25 INFO 139961471010624] Epoch[134] Batch[0] avg_epoch_loss=2.514423\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=2.51442265511\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:30 INFO 139961471010624] Epoch[134] Batch[5] avg_epoch_loss=2.470279\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:30 INFO 139961471010624] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=2.47027870019\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:30 INFO 139961471010624] Epoch[134] Batch [5]#011Speed: 62.27 samples/sec#011loss=2.470279\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:34 INFO 139961471010624] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14516.695022583008, \"sum\": 14516.695022583008, \"min\": 14516.695022583008}}, \"EndTime\": 1587682234.761521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682220.244391}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:34 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9869770837 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:34 INFO 139961471010624] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=134, train loss <loss>=2.44184885025\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:34 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:39 INFO 139961471010624] Epoch[135] Batch[0] avg_epoch_loss=2.569135\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:39 INFO 139961471010624] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=2.56913542747\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:45 INFO 139961471010624] Epoch[135] Batch[5] avg_epoch_loss=2.490369\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:45 INFO 139961471010624] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=2.49036912123\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:45 INFO 139961471010624] Epoch[135] Batch [5]#011Speed: 62.21 samples/sec#011loss=2.490369\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:49 INFO 139961471010624] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14568.403005599976, \"sum\": 14568.403005599976, \"min\": 14568.403005599976}}, \"EndTime\": 1587682249.330332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682234.761598}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:49 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.6262174572 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:49 INFO 139961471010624] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=135, train loss <loss>=2.50088694096\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:49 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:54 INFO 139961471010624] Epoch[136] Batch[0] avg_epoch_loss=2.665158\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:54 INFO 139961471010624] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=2.66515827179\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:59 INFO 139961471010624] Epoch[136] Batch[5] avg_epoch_loss=2.555275\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:59 INFO 139961471010624] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=2.55527504285\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:50:59 INFO 139961471010624] Epoch[136] Batch [5]#011Speed: 62.36 samples/sec#011loss=2.555275\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] Epoch[136] Batch[10] avg_epoch_loss=2.567962\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=2.58318729401\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] Epoch[136] Batch [10]#011Speed: 61.40 samples/sec#011loss=2.583187\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15600.069999694824, \"sum\": 15600.069999694824, \"min\": 15600.069999694824}}, \"EndTime\": 1587682264.930981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682249.330397}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.9226440567 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] #quality_metric: host=algo-1, epoch=136, train loss <loss>=2.56796242974\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:04 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:10 INFO 139961471010624] Epoch[137] Batch[0] avg_epoch_loss=2.497956\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=2.49795603752\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:15 INFO 139961471010624] Epoch[137] Batch[5] avg_epoch_loss=2.450594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:15 INFO 139961471010624] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=2.45059367021\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:15 INFO 139961471010624] Epoch[137] Batch [5]#011Speed: 61.40 samples/sec#011loss=2.450594\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] Epoch[137] Batch[10] avg_epoch_loss=2.515087\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=2.59247922897\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] Epoch[137] Batch [10]#011Speed: 60.48 samples/sec#011loss=2.592479\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15776.547908782959, \"sum\": 15776.547908782959, \"min\": 15776.547908782959}}, \"EndTime\": 1587682280.708001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682264.931042}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=42.7847840381 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=137, train loss <loss>=2.51508710601\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:20 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:25 INFO 139961471010624] Epoch[138] Batch[0] avg_epoch_loss=2.601866\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=2.60186576843\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:31 INFO 139961471010624] Epoch[138] Batch[5] avg_epoch_loss=2.535880\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:31 INFO 139961471010624] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=2.53587969144\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:31 INFO 139961471010624] Epoch[138] Batch [5]#011Speed: 62.02 samples/sec#011loss=2.535880\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:35 INFO 139961471010624] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14592.56911277771, \"sum\": 14592.56911277771, \"min\": 14592.56911277771}}, \"EndTime\": 1587682295.300963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682280.708059}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:35 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.4591364968 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:35 INFO 139961471010624] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:35 INFO 139961471010624] #quality_metric: host=algo-1, epoch=138, train loss <loss>=2.49708898067\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:35 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:40 INFO 139961471010624] Epoch[139] Batch[0] avg_epoch_loss=2.514708\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=2.51470828056\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:45 INFO 139961471010624] Epoch[139] Batch[5] avg_epoch_loss=2.456611\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:45 INFO 139961471010624] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=2.4566111962\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:45 INFO 139961471010624] Epoch[139] Batch [5]#011Speed: 61.93 samples/sec#011loss=2.456611\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:49 INFO 139961471010624] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14673.033952713013, \"sum\": 14673.033952713013, \"min\": 14673.033952713013}}, \"EndTime\": 1587682309.974398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682295.301042}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:49 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=41.7087899417 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:49 INFO 139961471010624] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=139, train loss <loss>=2.41871038675\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:49 INFO 139961471010624] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/23/2020 22:51:55 INFO 139961471010624] Epoch[140] Batch[0] avg_epoch_loss=2.538957\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:51:55 INFO 139961471010624] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=2.53895688057\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:00 INFO 139961471010624] Epoch[140] Batch[5] avg_epoch_loss=2.526720\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:00 INFO 139961471010624] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=2.52672028542\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:00 INFO 139961471010624] Epoch[140] Batch [5]#011Speed: 62.00 samples/sec#011loss=2.526720\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] Epoch[140] Batch[10] avg_epoch_loss=2.544873\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=2.56665525436\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] Epoch[140] Batch [10]#011Speed: 61.45 samples/sec#011loss=2.566655\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15708.613157272339, \"sum\": 15708.613157272339, \"min\": 15708.613157272339}}, \"EndTime\": 1587682325.683462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682309.97446}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9963857097 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] #quality_metric: host=algo-1, epoch=140, train loss <loss>=2.54487254403\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:05 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:10 INFO 139961471010624] Epoch[141] Batch[0] avg_epoch_loss=2.608854\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:10 INFO 139961471010624] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=2.60885429382\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:16 INFO 139961471010624] Epoch[141] Batch[5] avg_epoch_loss=2.508449\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:16 INFO 139961471010624] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=2.5084493955\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:16 INFO 139961471010624] Epoch[141] Batch [5]#011Speed: 62.13 samples/sec#011loss=2.508449\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:20 INFO 139961471010624] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14654.443979263306, \"sum\": 14654.443979263306, \"min\": 14654.443979263306}}, \"EndTime\": 1587682340.338341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682325.683519}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:20 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=40.9429457704 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:20 INFO 139961471010624] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:20 INFO 139961471010624] #quality_metric: host=algo-1, epoch=141, train loss <loss>=2.48983905315\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:20 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:25 INFO 139961471010624] Epoch[142] Batch[0] avg_epoch_loss=2.444108\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:25 INFO 139961471010624] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=2.44410777092\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:30 INFO 139961471010624] Epoch[142] Batch[5] avg_epoch_loss=2.500296\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:30 INFO 139961471010624] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=2.50029567877\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:30 INFO 139961471010624] Epoch[142] Batch [5]#011Speed: 61.86 samples/sec#011loss=2.500296\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:34 INFO 139961471010624] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14659.702777862549, \"sum\": 14659.702777862549, \"min\": 14659.702777862549}}, \"EndTime\": 1587682354.998536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682340.338402}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:34 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=43.5885975534 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:34 INFO 139961471010624] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:34 INFO 139961471010624] #quality_metric: host=algo-1, epoch=142, train loss <loss>=2.52954211235\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:34 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:40 INFO 139961471010624] Epoch[143] Batch[0] avg_epoch_loss=2.559489\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:40 INFO 139961471010624] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=2.5594894886\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:45 INFO 139961471010624] Epoch[143] Batch[5] avg_epoch_loss=2.619221\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:45 INFO 139961471010624] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=2.61922125022\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:45 INFO 139961471010624] Epoch[143] Batch [5]#011Speed: 61.88 samples/sec#011loss=2.619221\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14646.17919921875, \"sum\": 14646.17919921875, \"min\": 14646.17919921875}}, \"EndTime\": 1587682369.645187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682354.998599}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] #throughput_metric: host=algo-1, train throughput=39.7370713075 records/second\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] #quality_metric: host=algo-1, epoch=143, train loss <loss>=2.56208446026\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] Loading parameters from best epoch (103)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 110.93497276306152, \"sum\": 110.93497276306152, \"min\": 110.93497276306152}}, \"EndTime\": 1587682369.756549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682369.645251}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] stopping training now\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] Final loss: 2.40773061189 (occurred at epoch 103)\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] #quality_metric: host=algo-1, train final_loss <loss>=2.40773061189\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 WARNING 139961471010624] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 WARNING 139961471010624] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:52:49 INFO 139961471010624] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 32846.739053726196, \"sum\": 32846.739053726196, \"min\": 32846.739053726196}}, \"EndTime\": 1587682402.604056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682369.756624}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:23 INFO 139961471010624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 33931.27512931824, \"sum\": 33931.27512931824, \"min\": 33931.27512931824}}, \"EndTime\": 1587682403.688549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682402.60548}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:23 INFO 139961471010624] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:23 INFO 139961471010624] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 143.10288429260254, \"sum\": 143.10288429260254, \"min\": 143.10288429260254}}, \"EndTime\": 1587682403.8318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682403.688605}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:23 INFO 139961471010624] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:23 INFO 139961471010624] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03886222839355469, \"sum\": 0.03886222839355469, \"min\": 0.03886222839355469}}, \"EndTime\": 1587682403.832563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682403.83185}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-23 22:53:45 Uploading - Uploading generated training model\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 16841.02702140808, \"sum\": 16841.02702140808, \"min\": 16841.02702140808}}, \"EndTime\": 1587682420.673546, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682403.832611}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, RMSE): 40.1434449833\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, mean_absolute_QuantileLoss): 16355.638567985097\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, mean_wQuantileLoss): 0.13587803080489402\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.1]): 0.07697326526651156\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.2]): 0.12001783375002434\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.3]): 0.14707989335099733\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.4]): 0.1639431604935876\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.5]): 0.17330593364714802\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.6]): 0.17206212895251471\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.7]): 0.1586151422683391\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.8]): 0.13082820393654496\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #test_score (algo-1, wQuantileLoss[0.9]): 0.08007671557837885\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.135878030805\u001b[0m\n",
      "\u001b[34m[04/23/2020 22:53:40 INFO 139961471010624] #quality_metric: host=algo-1, test RMSE <loss>=40.1434449833\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2265641.03102684, \"sum\": 2265641.03102684, \"min\": 2265641.03102684}, \"setuptime\": {\"count\": 1, \"max\": 9.032011032104492, \"sum\": 9.032011032104492, \"min\": 9.032011032104492}}, \"EndTime\": 1587682420.951903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587682420.673607}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-23 22:53:57 Completed - Training job completed\n",
      "Training seconds: 2332\n",
      "Billable seconds: 2332\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code name for now as we stopped the notebook.  \n",
    "# If you do this in a single sitting, you don't need to hard code\n",
    "# job_name = 'deepar-biketrain-with-categories-2018-12-21-04-05-44-478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-biketrain-no-categories-2020-04-23-22-12-59-148\n"
     ]
    }
   ],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: deepar-biketrain-no-categories-2020-04-23-22-12-59-148\n"
     ]
    }
   ],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to terminate the end point after completing the demo\n",
    "# Otherwise, you account will accumulate hourly charges\n",
    "\n",
    "# you can delete from sagemaker management console or through command line or throught code\n",
    "\n",
    "# sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
